{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 960\n",
      "CUDA version: 11.7\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "from os import sys\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import SegformerImageProcessor\n",
    "\n",
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('./src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_prepossessing import create_datasets_for_plants, get_labels\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"SegformerFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = SegformerImageProcessor.from_pretrained(checkpoint)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(num_labels, eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try to remove eval_accumulation_steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=25,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=1,\n",
    "    # eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer(model, num_labels, train_ds, test_ds) :\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(num_labels, eval_pred),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_of_type_for_crop(model_type, crop):\n",
    "    model_plant_names = [crop] + weed_plants\n",
    "    train_ds, val_ds, test_ds = create_datasets_for_plants(model_plant_names, model_type, crop)\n",
    "\n",
    "    print(\"Training subset number of images: \" + str(train_ds.num_rows))\n",
    "    print(\"Validation subset number of images: \" + str(val_ds.num_rows))\n",
    "    print(\"Test subset number of images: \" + str(test_ds.num_rows))\n",
    "\n",
    "    train_ds.set_transform(train_transforms)\n",
    "    val_ds.set_transform(train_transforms)\n",
    "    test_ds.set_transform(train_transforms)\n",
    "\n",
    "    id2label, label2id = get_labels(crop, model_type)\n",
    "\n",
    "    print('Number of classes:', len(id2label))\n",
    "    print('id2label:', id2label)\n",
    "    print('label2id:', label2id)\n",
    "\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n",
    "    trainer = initialize_trainer(model, len(id2label), train_ds, val_ds)\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the trained model, so that it can be used for inference later.\n",
    "    # Save the log history, so that it can be used for plotting later.\n",
    "    trainer.save_model('models/' + model_type + '/' + crop)\n",
    "    with open('models/' + model_type + '/' + crop + '/log_history.json', 'w') as file:\n",
    "        log_history = trainer.state.log_history\n",
    "        json.dump(log_history, file)\n",
    "\n",
    "    test_metric = trainer.evaluate(test_ds)\n",
    "    test_metric\n",
    "\n",
    "    with open('models/' + model_type + '/' + crop + '/test_metric.json', 'w') as file:\n",
    "        json.dump(test_metric, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_00173.png', 'img_00174.png', 'img_00175.png', 'img_00176.png', 'img_00177.png', 'img_00178.png', 'img_00672.png', 'img_00673.png', 'img_00674.png', 'img_00675.png', 'img_00676.png', 'img_00677.png', 'img_00678.png', 'img_00679.png', 'img_00680.png', 'img_00681.png', 'img_00682.png', 'img_00683.png', 'img_00684.png', 'img_00882.png', 'img_00883.png', 'img_00884.png', 'img_00885.png', 'img_00886.png', 'img_00887.png', 'img_00938.png', 'img_00980.png', 'img_00981.png', 'img_00982.png', 'img_00983.png', 'img_00984.png', 'img_00985.png', 'img_00986.png', 'img_00987.png', 'img_00988.png', 'img_00989.png', 'img_01070.png', 'img_01071.png', 'img_01072.png', 'img_01073.png', 'img_01074.png', 'img_01075.png', 'img_01076.png', 'img_01077.png', 'img_01078.png', 'img_01079.png', 'img_01219.png', 'img_01220.png', 'img_01221.png', 'img_01222.png', 'img_01223.png', 'img_01224.png', 'img_01225.png', 'img_01226.png', 'img_01227.png', 'img_01228.png', 'img_01279.png', 'img_01280.png', 'img_01281.png', 'img_01282.png', 'img_01283.png', 'img_01284.png', 'img_01285.png', 'img_01286.png', 'img_01287.png', 'img_01288.png', 'img_01455.png', 'img_01456.png', 'img_01457.png', 'img_01458.png', 'img_01459.png', 'img_01460.png', 'img_01461.png', 'img_01462.png', 'img_01463.png', 'img_01464.png', 'img_01465.png', 'img_01466.png', 'img_01467.png', 'img_01468.png', 'img_01469.png', 'img_01825.png', 'img_01826.png', 'img_01827.png', 'img_01828.png', 'img_01829.png', 'img_01830.png', 'img_01831.png', 'img_01832.png', 'img_01833.png', 'img_01834.png', 'img_01835.png', 'img_01836.png', 'img_01837.png', 'img_01928.png', 'img_01929.png', 'img_01930.png', 'img_01931.png', 'img_01932.png', 'img_01933.png', 'img_01934.png', 'img_01935.png', 'img_01936.png', 'img_01937.png', 'img_01938.png', 'img_01939.png', 'img_01940.png', 'img_01941.png', 'img_01942.png', 'img_01943.png', 'img_01944.png', 'img_01945.png', 'img_01946.png', 'img_01947.png', 'img_01948.png', 'img_01949.png', 'img_01950.png', 'img_01951.png', 'img_01952.png', 'img_01953.png', 'img_01954.png', 'img_01955.png', 'img_02099.png', 'img_02100.png', 'img_02101.png', 'img_02102.png', 'img_02103.png', 'img_02104.png', 'img_02105.png', 'img_02106.png', 'img_02107.png', 'img_02108.png', 'img_02109.png', 'img_02110.png', 'img_02111.png', 'img_02112.png', 'img_02113.png', 'img_02114.png', 'img_02115.png', 'img_02116.png', 'img_02117.png', 'img_02118.png', 'img_02119.png', 'img_02120.png', 'img_02121.png', 'img_02122.png', 'img_02123.png', 'img_02124.png', 'img_02125.png', 'img_02126.png', 'img_02127.png', 'img_02128.png', 'img_02129.png', 'img_02130.png', 'img_02131.png', 'img_02132.png', 'img_02133.png', 'img_02134.png', 'img_02135.png', 'img_02136.png', 'img_02137.png', 'img_02138.png', 'img_02139.png', 'img_02140.png', 'img_02348.png', 'img_02349.png', 'img_02350.png', 'img_02351.png', 'img_02352.png', 'img_02353.png', 'img_02354.png', 'img_02355.png', 'img_02356.png', 'img_02357.png', 'img_02358.png', 'img_02359.png', 'img_02360.png', 'img_02361.png', 'img_02362.png', 'img_02363.png', 'img_02364.png', 'img_02365.png', 'img_02366.png', 'img_02367.png', 'img_02368.png', 'img_02369.png', 'img_02370.png', 'img_02371.png', 'img_02372.png', 'img_02373.png', 'img_02374.png', 'img_02375.png', 'img_02376.png', 'img_02377.png', 'img_02378.png', 'img_02379.png', 'img_02380.png', 'img_02381.png', 'img_02382.png', 'img_02383.png', 'img_02384.png', 'img_02385.png', 'img_02543.png', 'img_02544.png', 'img_02545.png', 'img_02546.png', 'img_02547.png', 'img_02548.png', 'img_02549.png', 'img_02550.png']\n",
      "Number of plant images for plant broad_bean : 210\n",
      "['img_01790.png', 'img_01799.png', 'img_01804.png', 'img_01807.png', 'img_01796.png', 'img_01788.png', 'img_01803.png', 'img_01802.png', 'img_01806.png', 'img_01801.png', 'img_01794.png', 'img_01789.png', 'img_01793.png', 'img_01795.png', 'img_01805.png', 'img_01792.png', 'img_01797.png', 'img_01800.png', 'img_01791.png', 'img_01798.png']\n",
      "Number of plant images for plant corn_spurry : 20\n",
      "['img_01796.png', 'img_01767.png', 'img_01768.png']\n",
      "Number of plant images for plant red-root_amaranth : 3\n",
      "['img_01813.png', 'img_01812.png', 'img_01809.png', 'img_01808.png', 'img_01810.png', 'img_01811.png', 'img_01815.png', 'img_01814.png']\n",
      "Number of plant images for plant red_fingergrass : 8\n",
      "['img_00712.png', 'img_00659.png', 'img_00209.png', 'img_00657.png', 'img_00419.png', 'img_00208.png', 'img_00717.png', 'img_00424.png', 'img_00204.png', 'img_00658.png', 'img_00715.png', 'img_00206.png', 'img_00421.png', 'img_00203.png', 'img_00434.png', 'img_00713.png', 'img_00430.png', 'img_00420.png', 'img_00710.png', 'img_00716.png', 'img_00709.png', 'img_00429.png', 'img_00431.png', 'img_00435.png', 'img_00714.png']\n",
      "Number of plant images for plant common_wild_oat : 25\n",
      "['img_01259.png', 'img_00465.png', 'img_00857.png', 'img_00377.png', 'img_00076.png', 'img_01263.png', 'img_00222.png', 'img_00617.png', 'img_00612.png', 'img_01145.png', 'img_00953.png', 'img_00440.png', 'img_00907.png', 'img_00362.png', 'img_00223.png', 'img_00558.png', 'img_01262.png', 'img_00569.png', 'img_01754.png', 'img_01147.png', 'img_00375.png', 'img_00361.png', 'img_01148.png', 'img_00812.png', 'img_00450.png', 'img_00370.png', 'img_00364.png', 'img_01766.png', 'img_00855.png', 'img_00116.png', 'img_00369.png', 'img_01270.png', 'img_00121.png', 'img_00811.png', 'img_01762.png', 'img_00954.png', 'img_00608.png', 'img_00077.png', 'img_00079.png', 'img_01141.png', 'img_01049.png', 'img_00611.png', 'img_00613.png', 'img_01197.png', 'img_01140.png', 'img_00619.png', 'img_00219.png', 'img_00810.png', 'img_01142.png', 'img_00620.png', 'img_01752.png', 'img_01194.png', 'img_00454.png', 'img_00374.png', 'img_00955.png', 'img_00123.png', 'img_01196.png', 'img_01144.png', 'img_01264.png', 'img_00366.png', 'img_00118.png', 'img_00363.png', 'img_00086.png', 'img_01044.png', 'img_01198.png', 'img_01751.png', 'img_00561.png', 'img_00224.png', 'img_01042.png', 'img_01266.png', 'img_01046.png', 'img_00556.png', 'img_01190.png', 'img_00618.png', 'img_00804.png', 'img_01261.png', 'img_00457.png', 'img_01191.png', 'img_00555.png', 'img_00082.png', 'img_00448.png', 'img_00456.png', 'img_01045.png', 'img_00447.png', 'img_01767.png', 'img_00609.png', 'img_00073.png', 'img_01043.png', 'img_00856.png', 'img_00957.png', 'img_01758.png', 'img_00078.png', 'img_00371.png', 'img_00616.png', 'img_01756.png', 'img_01048.png', 'img_00225.png', 'img_01753.png', 'img_01267.png', 'img_01871.png', 'img_01260.png', 'img_00615.png', 'img_01750.png', 'img_01768.png', 'img_00959.png', 'img_00956.png', 'img_00610.png', 'img_01765.png', 'img_01757.png', 'img_00068.png', 'img_01877.png', 'img_01760.png', 'img_01271.png', 'img_00360.png', 'img_00228.png', 'img_01759.png', 'img_00365.png', 'img_00122.png', 'img_01764.png', 'img_00466.png', 'img_01146.png', 'img_00449.png', 'img_00614.png', 'img_00372.png', 'img_00726.png', 'img_00378.png', 'img_01189.png', 'img_00958.png', 'img_01755.png', 'img_01763.png', 'img_00081.png', 'img_00070.png', 'img_00936.png', 'img_00821.png', 'img_00367.png', 'img_00859.png', 'img_01047.png', 'img_01265.png', 'img_00359.png', 'img_00952.png', 'img_00075.png', 'img_00080.png', 'img_01041.png', 'img_00069.png', 'img_00368.png', 'img_01143.png', 'img_00376.png', 'img_01040.png', 'img_00452.png', 'img_00950.png', 'img_01192.png', 'img_00373.png', 'img_00729.png', 'img_01193.png', 'img_00451.png', 'img_00567.png', 'img_01195.png', 'img_01761.png', 'img_01268.png', 'img_00951.png', 'img_00858.png', 'img_01873.png']\n",
      "Number of plant images for plant cornflower : 162\n",
      "['img_01349.png', 'img_02495.png', 'img_00625.png', 'img_01000.png', 'img_00626.png', 'img_01128.png', 'img_02199.png', 'img_01600.png', 'img_01580.png', 'img_02010.png', 'img_01105.png', 'img_02204.png', 'img_01720.png', 'img_01003.png', 'img_01250.png', 'img_01635.png', 'img_01171.png', 'img_01009.png', 'img_00750.png', 'img_02190.png', 'img_01005.png', 'img_02006.png', 'img_01254.png', 'img_01646.png', 'img_00623.png', 'img_01007.png', 'img_01255.png', 'img_01641.png', 'img_02491.png', 'img_02005.png', 'img_01992.png', 'img_01002.png', 'img_01634.png', 'img_02202.png', 'img_00624.png', 'img_02003.png', 'img_01249.png', 'img_01597.png', 'img_01709.png', 'img_00631.png', 'img_01630.png', 'img_01998.png', 'img_01722.png', 'img_02494.png', 'img_01100.png', 'img_00272.png', 'img_01124.png', 'img_02011.png', 'img_01594.png', 'img_00756.png', 'img_01632.png', 'img_01129.png', 'img_00768.png', 'img_01716.png', 'img_02200.png', 'img_01103.png', 'img_01253.png', 'img_01643.png', 'img_01354.png', 'img_00627.png', 'img_00860.png', 'img_01633.png', 'img_00866.png', 'img_00639.png', 'img_02201.png', 'img_00752.png', 'img_01642.png', 'img_01645.png', 'img_01713.png', 'img_01715.png', 'img_02198.png', 'img_00262.png', 'img_00862.png', 'img_00494.png', 'img_01177.png', 'img_01256.png', 'img_01102.png', 'img_01107.png', 'img_01601.png', 'img_02007.png', 'img_01258.png', 'img_01714.png', 'img_00633.png', 'img_01996.png', 'img_01257.png', 'img_01348.png', 'img_01123.png', 'img_02192.png', 'img_01108.png', 'img_01724.png', 'img_02490.png', 'img_00918.png', 'img_00621.png', 'img_00635.png', 'img_01004.png', 'img_01121.png', 'img_01989.png', 'img_01172.png', 'img_02195.png', 'img_00263.png', 'img_01712.png', 'img_01599.png', 'img_00270.png', 'img_01101.png', 'img_00637.png', 'img_01126.png', 'img_01106.png', 'img_00861.png', 'img_01725.png', 'img_02012.png', 'img_01243.png', 'img_00632.png', 'img_01592.png', 'img_01006.png', 'img_02203.png', 'img_01994.png', 'img_00758.png', 'img_01637.png', 'img_01109.png', 'img_01644.png', 'img_01995.png', 'img_00629.png', 'img_00622.png', 'img_01251.png', 'img_00261.png', 'img_01178.png', 'img_01990.png', 'img_00630.png', 'img_01993.png', 'img_01647.png', 'img_01252.png', 'img_00503.png', 'img_00863.png', 'img_00492.png', 'img_00753.png', 'img_01596.png', 'img_00638.png', 'img_01631.png', 'img_00759.png', 'img_01999.png', 'img_01717.png', 'img_01104.png', 'img_01122.png', 'img_01710.png', 'img_01638.png', 'img_02009.png', 'img_02489.png', 'img_01595.png', 'img_01997.png', 'img_00770.png', 'img_00252.png', 'img_00755.png', 'img_01169.png', 'img_01246.png', 'img_01173.png', 'img_01629.png', 'img_01711.png', 'img_01723.png', 'img_00628.png', 'img_02197.png', 'img_00864.png', 'img_01170.png', 'img_02193.png', 'img_01648.png', 'img_01175.png', 'img_01598.png', 'img_01640.png', 'img_01719.png', 'img_01721.png', 'img_00253.png', 'img_02004.png', 'img_01008.png', 'img_01176.png', 'img_01127.png', 'img_02492.png', 'img_01636.png', 'img_00636.png', 'img_01356.png', 'img_01593.png', 'img_00493.png', 'img_01174.png', 'img_02000.png', 'img_01120.png', 'img_02191.png', 'img_02493.png', 'img_01001.png', 'img_00634.png', 'img_01639.png', 'img_02008.png', 'img_02194.png', 'img_00865.png', 'img_02002.png', 'img_02001.png', 'img_01991.png', 'img_01570.png', 'img_01718.png', 'img_01125.png', 'img_00501.png', 'img_00867.png', 'img_02196.png']\n",
      "Number of plant images for plant corn_cockle : 200\n",
      "['img_01742.png', 'img_02013.png', 'img_02024.png', 'img_02218.png', 'img_01614.png', 'img_01731.png', 'img_02026.png', 'img_02216.png', 'img_01744.png', 'img_01653.png', 'img_01737.png', 'img_01543.png', 'img_01612.png', 'img_02226.png', 'img_01743.png', 'img_01650.png', 'img_01664.png', 'img_01659.png', 'img_01682.png', 'img_01651.png', 'img_01699.png', 'img_01667.png', 'img_02019.png', 'img_02205.png', 'img_01617.png', 'img_01627.png', 'img_01665.png', 'img_01728.png', 'img_01739.png', 'img_01658.png', 'img_01606.png', 'img_02214.png', 'img_02498.png', 'img_01734.png', 'img_01655.png', 'img_01727.png', 'img_01021.png', 'img_02222.png', 'img_01619.png', 'img_01623.png', 'img_01858.png', 'img_01863.png', 'img_02017.png', 'img_01652.png', 'img_01624.png', 'img_02501.png', 'img_02213.png', 'img_01656.png', 'img_02215.png', 'img_02221.png', 'img_01662.png', 'img_02483.png', 'img_02208.png', 'img_01622.png', 'img_02502.png', 'img_01615.png', 'img_01733.png', 'img_01657.png', 'img_01736.png', 'img_02028.png', 'img_02476.png', 'img_01747.png', 'img_01745.png', 'img_01610.png', 'img_01701.png', 'img_02499.png', 'img_01654.png', 'img_01707.png', 'img_02212.png', 'img_01663.png', 'img_02016.png', 'img_02025.png', 'img_02470.png', 'img_01729.png', 'img_02022.png', 'img_01748.png', 'img_02228.png', 'img_02029.png', 'img_01730.png', 'img_01625.png', 'img_02211.png', 'img_01670.png', 'img_01740.png', 'img_02015.png', 'img_02207.png', 'img_02229.png', 'img_02497.png', 'img_01669.png', 'img_01649.png', 'img_01732.png', 'img_02454.png', 'img_02477.png', 'img_02021.png', 'img_01022.png', 'img_01689.png', 'img_02020.png', 'img_01726.png', 'img_01738.png', 'img_02488.png', 'img_01861.png', 'img_01024.png', 'img_02223.png', 'img_01735.png', 'img_01668.png', 'img_02014.png', 'img_01026.png', 'img_02220.png', 'img_01559.png', 'img_01020.png', 'img_02227.png', 'img_01602.png', 'img_01609.png', 'img_02217.png', 'img_01025.png', 'img_02496.png', 'img_02219.png', 'img_01628.png', 'img_01706.png', 'img_01741.png', 'img_02210.png', 'img_02500.png', 'img_02027.png', 'img_01608.png', 'img_01028.png', 'img_01702.png', 'img_01856.png', 'img_01746.png', 'img_01618.png', 'img_02209.png', 'img_02453.png', 'img_01616.png', 'img_01621.png', 'img_01027.png', 'img_01620.png', 'img_02224.png', 'img_01537.png', 'img_02206.png', 'img_01611.png', 'img_01613.png', 'img_01029.png', 'img_01605.png', 'img_01607.png', 'img_01603.png', 'img_01604.png', 'img_02030.png', 'img_01666.png', 'img_02018.png', 'img_01023.png', 'img_01626.png', 'img_02023.png', 'img_01749.png', 'img_01660.png', 'img_01661.png', 'img_02225.png']\n",
      "Number of plant images for plant milk_thistle : 154\n",
      "['img_01276.png', 'img_00963.png', 'img_00961.png', 'img_01054.png', 'img_01155.png', 'img_01157.png', 'img_01156.png', 'img_01154.png', 'img_01200.png', 'img_01769.png', 'img_01150.png', 'img_00967.png', 'img_01050.png', 'img_01158.png', 'img_01201.png', 'img_01275.png', 'img_01056.png', 'img_00965.png', 'img_01208.png', 'img_01773.png', 'img_01052.png', 'img_01771.png', 'img_01055.png', 'img_01152.png', 'img_00966.png', 'img_01199.png', 'img_01206.png', 'img_01269.png', 'img_01770.png', 'img_01202.png', 'img_01774.png', 'img_01153.png', 'img_00962.png', 'img_01059.png', 'img_01273.png', 'img_01776.png', 'img_01149.png', 'img_01207.png', 'img_01772.png', 'img_00960.png', 'img_01203.png', 'img_01271.png', 'img_00937.png', 'img_01277.png', 'img_00968.png', 'img_01278.png', 'img_01051.png', 'img_01777.png', 'img_01272.png', 'img_01058.png', 'img_01270.png', 'img_01775.png', 'img_01057.png', 'img_00964.png', 'img_01053.png', 'img_01151.png', 'img_01205.png', 'img_01204.png', 'img_01274.png', 'img_00969.png']\n",
      "Number of plant images for plant rye_brome : 60\n",
      "['img_01781.png', 'img_01786.png', 'img_00645.png', 'img_01783.png', 'img_00646.png', 'img_00647.png', 'img_01804.png', 'img_00649.png', 'img_00651.png', 'img_00650.png', 'img_01806.png', 'img_00648.png', 'img_00652.png', 'img_01782.png', 'img_01784.png', 'img_01779.png', 'img_00643.png', 'img_01785.png', 'img_01787.png', 'img_00644.png', 'img_01778.png', 'img_01780.png']\n",
      "Number of plant images for plant narrow-leaved_plantain : 22\n",
      "['img_00474.png', 'img_00796.png', 'img_00536.png', 'img_00144.png', 'img_00601.png', 'img_00532.png', 'img_00605.png', 'img_00745.png', 'img_00914.png', 'img_00528.png', 'img_00356.png', 'img_00346.png', 'img_00595.png', 'img_00148.png', 'img_00145.png', 'img_00354.png', 'img_00542.png', 'img_00096.png', 'img_00149.png', 'img_00351.png', 'img_00852.png', 'img_00478.png', 'img_00607.png', 'img_00479.png', 'img_00486.png', 'img_00357.png', 'img_00350.png', 'img_00737.png', 'img_00347.png', 'img_00348.png', 'img_00476.png', 'img_00742.png', 'img_00736.png', 'img_00468.png', 'img_00546.png', 'img_00482.png', 'img_00469.png', 'img_00529.png', 'img_00853.png', 'img_00547.png', 'img_00341.png', 'img_00352.png', 'img_00477.png', 'img_00850.png', 'img_00467.png', 'img_00147.png', 'img_00353.png', 'img_00480.png', 'img_00596.png', 'img_00358.png', 'img_00150.png', 'img_00342.png', 'img_00599.png', 'img_00851.png', 'img_00343.png', 'img_00355.png', 'img_00600.png', 'img_00791.png', 'img_00344.png', 'img_00854.png', 'img_00604.png', 'img_00345.png', 'img_00603.png', 'img_00146.png', 'img_00908.png', 'img_00602.png', 'img_00606.png', 'img_00598.png', 'img_00797.png', 'img_00597.png', 'img_00545.png', 'img_00349.png']\n",
      "Number of plant images for plant small-flower_geranium : 72\n",
      "Training subset number of images: 467\n",
      "Validation subset number of images: 232\n",
      "Test subset number of images: 237\n",
      "Number of classes: 13\n",
      "id2label: {0: 'void', 1: 'soil', 2: 'broad_bean', 3: 'corn_spurry', 4: 'red-root_amaranth', 5: 'red_fingergrass', 6: 'common_wild_oat', 7: 'cornflower', 8: 'corn_cockle', 9: 'milk_thistle', 10: 'rye_brome', 11: 'narrow-leaved_plantain', 12: 'small-flower_geranium'}\n",
      "label2id: {'void': 0, 'soil': 1, 'broad_bean': 2, 'corn_spurry': 3, 'red-root_amaranth': 4, 'red_fingergrass': 5, 'common_wild_oat': 6, 'cornflower': 7, 'corn_cockle': 8, 'milk_thistle': 9, 'rye_brome': 10, 'narrow-leaved_plantain': 11, 'small-flower_geranium': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.running_var', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 1/1475 [00:09<3:46:01,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3566, 'learning_rate': 5.9959322033898307e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1475 [00:14<2:51:23,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3155, 'learning_rate': 5.991864406779661e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1475 [00:20<2:33:43,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2867, 'learning_rate': 5.9877966101694917e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1475 [00:25<2:25:02,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2813, 'learning_rate': 5.983728813559322e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1475 [00:30<2:20:21,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.259, 'learning_rate': 5.979661016949153e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1475 [00:36<2:17:07,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2331, 'learning_rate': 5.975593220338983e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1475 [00:41<2:15:29,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2219, 'learning_rate': 5.971525423728814e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1475 [00:46<2:14:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2086, 'learning_rate': 5.967457627118644e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1475 [00:52<2:13:14,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2019, 'learning_rate': 5.963389830508475e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1475 [00:57<2:12:38,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.192, 'learning_rate': 5.959322033898305e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1475 [01:03<2:13:00,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1605, 'learning_rate': 5.955254237288136e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1475 [01:08<2:12:27,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1491, 'learning_rate': 5.951186440677966e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1475 [01:13<2:11:57,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1372, 'learning_rate': 5.947118644067797e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1475 [01:19<2:11:18,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1343, 'learning_rate': 5.943050847457627e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1475 [01:24<2:11:01,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1009, 'learning_rate': 5.9389830508474584e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/1475 [01:30<2:10:50,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0831, 'learning_rate': 5.934915254237288e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1475 [01:35<2:10:59,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0512, 'learning_rate': 5.930847457627119e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1475 [01:40<2:10:45,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0312, 'learning_rate': 5.926779661016949e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 19/1475 [01:46<2:10:39,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0488, 'learning_rate': 5.92271186440678e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 20/1475 [01:51<2:10:30,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0035, 'learning_rate': 5.91864406779661e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 21/1475 [01:57<2:10:32,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9973, 'learning_rate': 5.914576271186441e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 22/1475 [02:02<2:10:21,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9994, 'learning_rate': 5.910508474576271e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1475 [02:07<2:10:05,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9561, 'learning_rate': 5.906440677966102e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1475 [02:13<2:09:48,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9343, 'learning_rate': 5.902372881355933e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 25/1475 [02:18<2:09:38,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.892, 'learning_rate': 5.8983050847457634e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 26/1475 [02:23<2:10:13,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8725, 'learning_rate': 5.894237288135593e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1475 [02:29<2:10:05,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8801, 'learning_rate': 5.890169491525424e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1475 [02:34<2:09:40,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8483, 'learning_rate': 5.886101694915254e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 29/1475 [02:40<2:09:47,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.83, 'learning_rate': 5.882033898305085e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1475 [02:45<2:09:21,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7652, 'learning_rate': 5.877966101694915e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 31/1475 [02:50<2:09:51,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7702, 'learning_rate': 5.873898305084746e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 32/1475 [02:56<2:09:50,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7255, 'learning_rate': 5.869830508474576e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/1475 [03:01<2:10:08,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7311, 'learning_rate': 5.8657627118644074e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/1475 [03:07<2:09:49,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7232, 'learning_rate': 5.861694915254238e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 35/1475 [03:12<2:09:33,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6986, 'learning_rate': 5.8576271186440684e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 36/1475 [03:17<2:09:04,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6735, 'learning_rate': 5.853559322033899e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 37/1475 [03:23<2:09:02,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6152, 'learning_rate': 5.849491525423729e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model_of_type_for_crop(\u001b[39m\"\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbroad_bean\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mtrain_model_of_type_for_crop\u001b[1;34m(model_type, crop)\u001b[0m\n\u001b[0;32m     19\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSemanticSegmentation\u001b[39m.\u001b[39mfrom_pretrained(checkpoint, id2label\u001b[39m=\u001b[39mid2label, label2id\u001b[39m=\u001b[39mlabel2id)\n\u001b[0;32m     20\u001b[0m trainer \u001b[39m=\u001b[39m initialize_trainer(model, \u001b[39mlen\u001b[39m(id2label), train_ds, val_ds)\n\u001b[1;32m---> 21\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     23\u001b[0m \u001b[39m# Save the trained model, so that it can be used for inference later.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Save the log history, so that it can be used for plotting later.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m model_type \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m crop)\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\transformers\\trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1815\u001b[0m ):\n\u001b[0;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\transformers\\trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2663\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\accelerate\\accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1853\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kate\\AppData\\Local\\miniconda3\\envs\\master\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_of_type_for_crop(\"multiclass\", \"broad_bean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from typing import NoReturn\n",
    "\n",
    "# def shutdown_windows() -> NoReturn:\n",
    "#     subprocess.run([\"shutdown\", \"/s\", \"/t\", \"0\"])\n",
    "\n",
    "# shutdown_windows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
