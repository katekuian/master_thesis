{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 960\n",
      "CUDA version: 11.7\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "    print('Memory Usage:') \n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "from os import sys\n",
    "\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('./src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_prepossessing import create_datasets_for_plants, get_labels\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"SegformerFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = SegformerImageProcessor.from_pretrained(checkpoint)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(num_labels, eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer(model, num_labels, train_ds, test_ds) :\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(num_labels, eval_pred),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "\tmodel_type = \"multiclass\"\n",
    "\tcrop = \"broad_bean\" \n",
    "\tid2label, label2id = get_labels(crop, model_type)\n",
    "\treturn AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 16:40:17,862\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-09-09 16:40:17,930\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "optuna.__version__\n",
    "import sigopt\n",
    "# import wandb\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "def sigopt_hp_space(trial):\n",
    "    return [\n",
    "        {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"}\n",
    "    ]\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    }\n",
    "\n",
    "# def wandb_hp_space(trial):\n",
    "#     return {\n",
    "#         \"method\": \"random\",\n",
    "#         \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n",
    "#         \"parameters\": {\n",
    "#             \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4}\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "def ray_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-6, 1e-4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer_for_hp_search(num_labels, train_ds, test_ds) :\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(num_labels, eval_pred),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "        direction=\"minimize\",\n",
    "        backend=\"optuna\",\n",
    "        hp_space=optuna_hp_space,\n",
    "        n_trials=20,\n",
    "    )\n",
    "\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_00173.png', 'img_00174.png', 'img_00175.png', 'img_00176.png', 'img_00177.png', 'img_00178.png', 'img_00672.png', 'img_00673.png', 'img_00674.png', 'img_00675.png', 'img_00676.png', 'img_00677.png', 'img_00678.png', 'img_00679.png', 'img_00680.png', 'img_00681.png', 'img_00682.png', 'img_00683.png', 'img_00684.png', 'img_00882.png', 'img_00883.png', 'img_00884.png', 'img_00885.png', 'img_00886.png', 'img_00887.png', 'img_00938.png', 'img_00980.png', 'img_00981.png', 'img_00982.png', 'img_00983.png', 'img_00984.png', 'img_00985.png', 'img_00986.png', 'img_00987.png', 'img_00988.png', 'img_00989.png', 'img_01070.png', 'img_01071.png', 'img_01072.png', 'img_01073.png', 'img_01074.png', 'img_01075.png', 'img_01076.png', 'img_01077.png', 'img_01078.png', 'img_01079.png', 'img_01219.png', 'img_01220.png', 'img_01221.png', 'img_01222.png', 'img_01223.png', 'img_01224.png', 'img_01225.png', 'img_01226.png', 'img_01227.png', 'img_01228.png', 'img_01279.png', 'img_01280.png', 'img_01281.png', 'img_01282.png', 'img_01283.png', 'img_01284.png', 'img_01285.png', 'img_01286.png', 'img_01287.png', 'img_01288.png', 'img_01455.png', 'img_01456.png', 'img_01457.png', 'img_01458.png', 'img_01459.png', 'img_01460.png', 'img_01461.png', 'img_01462.png', 'img_01463.png', 'img_01464.png', 'img_01465.png', 'img_01466.png', 'img_01467.png', 'img_01468.png', 'img_01469.png', 'img_01825.png', 'img_01826.png', 'img_01827.png', 'img_01828.png', 'img_01829.png', 'img_01830.png', 'img_01831.png', 'img_01832.png', 'img_01833.png', 'img_01834.png', 'img_01835.png', 'img_01836.png', 'img_01837.png', 'img_01928.png', 'img_01929.png', 'img_01930.png', 'img_01931.png', 'img_01932.png', 'img_01933.png', 'img_01934.png', 'img_01935.png', 'img_01936.png', 'img_01937.png', 'img_01938.png', 'img_01939.png', 'img_01940.png', 'img_01941.png', 'img_01942.png', 'img_01943.png', 'img_01944.png', 'img_01945.png', 'img_01946.png', 'img_01947.png', 'img_01948.png', 'img_01949.png', 'img_01950.png', 'img_01951.png', 'img_01952.png', 'img_01953.png', 'img_01954.png', 'img_01955.png', 'img_02099.png', 'img_02100.png', 'img_02101.png', 'img_02102.png', 'img_02103.png', 'img_02104.png', 'img_02105.png', 'img_02106.png', 'img_02107.png', 'img_02108.png', 'img_02109.png', 'img_02110.png', 'img_02111.png', 'img_02112.png', 'img_02113.png', 'img_02114.png', 'img_02115.png', 'img_02116.png', 'img_02117.png', 'img_02118.png', 'img_02119.png', 'img_02120.png', 'img_02121.png', 'img_02122.png', 'img_02123.png', 'img_02124.png', 'img_02125.png', 'img_02126.png', 'img_02127.png', 'img_02128.png', 'img_02129.png', 'img_02130.png', 'img_02131.png', 'img_02132.png', 'img_02133.png', 'img_02134.png', 'img_02135.png', 'img_02136.png', 'img_02137.png', 'img_02138.png', 'img_02139.png', 'img_02140.png', 'img_02348.png', 'img_02349.png', 'img_02350.png', 'img_02351.png', 'img_02352.png', 'img_02353.png', 'img_02354.png', 'img_02355.png', 'img_02356.png', 'img_02357.png', 'img_02358.png', 'img_02359.png', 'img_02360.png', 'img_02361.png', 'img_02362.png', 'img_02363.png', 'img_02364.png', 'img_02365.png', 'img_02366.png', 'img_02367.png', 'img_02368.png', 'img_02369.png', 'img_02370.png', 'img_02371.png', 'img_02372.png', 'img_02373.png', 'img_02374.png', 'img_02375.png', 'img_02376.png', 'img_02377.png', 'img_02378.png', 'img_02379.png', 'img_02380.png', 'img_02381.png', 'img_02382.png', 'img_02383.png', 'img_02384.png', 'img_02385.png', 'img_02543.png', 'img_02544.png', 'img_02545.png', 'img_02546.png', 'img_02547.png', 'img_02548.png', 'img_02549.png', 'img_02550.png']\n",
      "Number of plant images for plant broad_bean : 210\n",
      "['img_01804.png', 'img_01807.png', 'img_01790.png', 'img_01803.png', 'img_01793.png', 'img_01797.png', 'img_01798.png', 'img_01796.png', 'img_01799.png', 'img_01795.png', 'img_01791.png', 'img_01802.png', 'img_01806.png', 'img_01789.png', 'img_01805.png', 'img_01800.png', 'img_01794.png', 'img_01801.png', 'img_01792.png', 'img_01788.png']\n",
      "Number of plant images for plant corn_spurry : 20\n",
      "['img_01768.png', 'img_01767.png', 'img_01796.png']\n",
      "Number of plant images for plant red-root_amaranth : 3\n",
      "['img_01812.png', 'img_01809.png', 'img_01815.png', 'img_01810.png', 'img_01808.png', 'img_01813.png', 'img_01814.png', 'img_01811.png']\n",
      "Number of plant images for plant red_fingergrass : 8\n",
      "['img_00434.png', 'img_00203.png', 'img_00424.png', 'img_00717.png', 'img_00657.png', 'img_00430.png', 'img_00715.png', 'img_00420.png', 'img_00658.png', 'img_00209.png', 'img_00659.png', 'img_00419.png', 'img_00709.png', 'img_00713.png', 'img_00206.png', 'img_00429.png', 'img_00712.png', 'img_00421.png', 'img_00710.png', 'img_00204.png', 'img_00431.png', 'img_00435.png', 'img_00208.png', 'img_00714.png', 'img_00716.png']\n",
      "Number of plant images for plant common_wild_oat : 25\n",
      "['img_00618.png', 'img_00078.png', 'img_00612.png', 'img_01764.png', 'img_01195.png', 'img_01271.png', 'img_00447.png', 'img_01762.png', 'img_00856.png', 'img_00613.png', 'img_00223.png', 'img_01198.png', 'img_01758.png', 'img_00373.png', 'img_01189.png', 'img_00614.png', 'img_01264.png', 'img_01192.png', 'img_01266.png', 'img_01270.png', 'img_00804.png', 'img_00569.png', 'img_01753.png', 'img_00374.png', 'img_00959.png', 'img_00364.png', 'img_01751.png', 'img_00620.png', 'img_01048.png', 'img_00857.png', 'img_00936.png', 'img_01196.png', 'img_01191.png', 'img_01143.png', 'img_00810.png', 'img_01752.png', 'img_00359.png', 'img_01871.png', 'img_00558.png', 'img_01263.png', 'img_00376.png', 'img_00450.png', 'img_00122.png', 'img_00858.png', 'img_00362.png', 'img_01194.png', 'img_00952.png', 'img_00378.png', 'img_01262.png', 'img_01755.png', 'img_00076.png', 'img_00954.png', 'img_00617.png', 'img_00371.png', 'img_00609.png', 'img_00118.png', 'img_00950.png', 'img_01197.png', 'img_00228.png', 'img_01754.png', 'img_00957.png', 'img_00457.png', 'img_01042.png', 'img_00466.png', 'img_00821.png', 'img_00465.png', 'img_00956.png', 'img_00073.png', 'img_00219.png', 'img_00556.png', 'img_00372.png', 'img_00224.png', 'img_01193.png', 'img_00367.png', 'img_01767.png', 'img_00608.png', 'img_01147.png', 'img_00079.png', 'img_00081.png', 'img_00368.png', 'img_01044.png', 'img_01259.png', 'img_00365.png', 'img_00082.png', 'img_00069.png', 'img_00859.png', 'img_01260.png', 'img_01190.png', 'img_01268.png', 'img_00448.png', 'img_01045.png', 'img_00440.png', 'img_00363.png', 'img_00369.png', 'img_00726.png', 'img_00452.png', 'img_00955.png', 'img_01148.png', 'img_00555.png', 'img_00812.png', 'img_01040.png', 'img_00370.png', 'img_00953.png', 'img_00222.png', 'img_01146.png', 'img_00123.png', 'img_01763.png', 'img_00075.png', 'img_00068.png', 'img_00449.png', 'img_01765.png', 'img_01766.png', 'img_00451.png', 'img_00907.png', 'img_01768.png', 'img_00456.png', 'img_01142.png', 'img_00377.png', 'img_00619.png', 'img_00611.png', 'img_00077.png', 'img_01759.png', 'img_01761.png', 'img_00360.png', 'img_01041.png', 'img_00086.png', 'img_01750.png', 'img_00951.png', 'img_01141.png', 'img_00561.png', 'img_00375.png', 'img_00615.png', 'img_00855.png', 'img_01046.png', 'img_01043.png', 'img_01757.png', 'img_00121.png', 'img_01049.png', 'img_01873.png', 'img_00454.png', 'img_00958.png', 'img_01756.png', 'img_01265.png', 'img_01145.png', 'img_01140.png', 'img_01760.png', 'img_01047.png', 'img_01267.png', 'img_00361.png', 'img_01877.png', 'img_00366.png', 'img_00811.png', 'img_00080.png', 'img_01261.png', 'img_00729.png', 'img_00070.png', 'img_00116.png', 'img_00567.png', 'img_00610.png', 'img_00616.png', 'img_00225.png', 'img_01144.png']\n",
      "Number of plant images for plant cornflower : 162\n",
      "['img_01993.png', 'img_00753.png', 'img_01719.png', 'img_01714.png', 'img_01348.png', 'img_01724.png', 'img_01128.png', 'img_02000.png', 'img_02001.png', 'img_00622.png', 'img_00636.png', 'img_01129.png', 'img_01104.png', 'img_01123.png', 'img_01109.png', 'img_01593.png', 'img_00759.png', 'img_01636.png', 'img_02194.png', 'img_00621.png', 'img_01633.png', 'img_01126.png', 'img_01249.png', 'img_01718.png', 'img_01251.png', 'img_01709.png', 'img_01349.png', 'img_01599.png', 'img_02490.png', 'img_02008.png', 'img_01256.png', 'img_01998.png', 'img_01175.png', 'img_02009.png', 'img_02492.png', 'img_00634.png', 'img_01721.png', 'img_01990.png', 'img_01997.png', 'img_02198.png', 'img_02495.png', 'img_01648.png', 'img_02193.png', 'img_01629.png', 'img_00627.png', 'img_01601.png', 'img_01992.png', 'img_01170.png', 'img_02012.png', 'img_00628.png', 'img_01580.png', 'img_01598.png', 'img_01177.png', 'img_00755.png', 'img_02203.png', 'img_01634.png', 'img_01632.png', 'img_02192.png', 'img_01005.png', 'img_02010.png', 'img_00626.png', 'img_01644.png', 'img_02005.png', 'img_01725.png', 'img_00639.png', 'img_00861.png', 'img_02190.png', 'img_00494.png', 'img_00863.png', 'img_01642.png', 'img_01647.png', 'img_02204.png', 'img_02202.png', 'img_01101.png', 'img_01720.png', 'img_01995.png', 'img_01646.png', 'img_00501.png', 'img_00270.png', 'img_01001.png', 'img_02011.png', 'img_01176.png', 'img_01246.png', 'img_00862.png', 'img_01102.png', 'img_01105.png', 'img_02195.png', 'img_02494.png', 'img_01108.png', 'img_01171.png', 'img_01637.png', 'img_00630.png', 'img_01178.png', 'img_02002.png', 'img_00631.png', 'img_01173.png', 'img_00272.png', 'img_01172.png', 'img_00262.png', 'img_00632.png', 'img_00758.png', 'img_00768.png', 'img_01258.png', 'img_01641.png', 'img_00866.png', 'img_01354.png', 'img_00624.png', 'img_00635.png', 'img_01723.png', 'img_02493.png', 'img_01125.png', 'img_01645.png', 'img_01711.png', 'img_02191.png', 'img_01722.png', 'img_00770.png', 'img_00918.png', 'img_02003.png', 'img_00864.png', 'img_01994.png', 'img_01999.png', 'img_00867.png', 'img_01004.png', 'img_01107.png', 'img_01120.png', 'img_01252.png', 'img_01631.png', 'img_01006.png', 'img_01643.png', 'img_00629.png', 'img_00637.png', 'img_01638.png', 'img_01121.png', 'img_01710.png', 'img_02491.png', 'img_01124.png', 'img_01174.png', 'img_01639.png', 'img_01003.png', 'img_00503.png', 'img_01570.png', 'img_01715.png', 'img_00633.png', 'img_01253.png', 'img_00623.png', 'img_01356.png', 'img_01103.png', 'img_01000.png', 'img_00493.png', 'img_01002.png', 'img_02196.png', 'img_00756.png', 'img_01008.png', 'img_01712.png', 'img_01169.png', 'img_01243.png', 'img_01009.png', 'img_01122.png', 'img_02004.png', 'img_00261.png', 'img_00865.png', 'img_01713.png', 'img_01250.png', 'img_01592.png', 'img_02006.png', 'img_02201.png', 'img_01257.png', 'img_01127.png', 'img_02197.png', 'img_01989.png', 'img_01594.png', 'img_02489.png', 'img_01106.png', 'img_01254.png', 'img_01630.png', 'img_01596.png', 'img_00860.png', 'img_01640.png', 'img_01635.png', 'img_00638.png', 'img_00263.png', 'img_01717.png', 'img_00625.png', 'img_01255.png', 'img_00252.png', 'img_00253.png', 'img_01597.png', 'img_00750.png', 'img_01716.png', 'img_00492.png', 'img_01595.png', 'img_00752.png', 'img_01007.png', 'img_01100.png', 'img_01600.png', 'img_01996.png', 'img_02199.png', 'img_02007.png', 'img_02200.png', 'img_01991.png']\n",
      "Number of plant images for plant corn_cockle : 200\n",
      "['img_02477.png', 'img_02225.png', 'img_01702.png', 'img_01605.png', 'img_02214.png', 'img_02499.png', 'img_01619.png', 'img_01626.png', 'img_01858.png', 'img_01662.png', 'img_01682.png', 'img_02025.png', 'img_02213.png', 'img_01024.png', 'img_01745.png', 'img_01856.png', 'img_02470.png', 'img_01610.png', 'img_01021.png', 'img_02020.png', 'img_01733.png', 'img_01665.png', 'img_01660.png', 'img_02022.png', 'img_01863.png', 'img_02029.png', 'img_01670.png', 'img_01611.png', 'img_02224.png', 'img_01620.png', 'img_02015.png', 'img_02014.png', 'img_02027.png', 'img_01861.png', 'img_02021.png', 'img_01537.png', 'img_02496.png', 'img_01739.png', 'img_02497.png', 'img_02222.png', 'img_01669.png', 'img_02220.png', 'img_01731.png', 'img_02023.png', 'img_02500.png', 'img_01732.png', 'img_01025.png', 'img_01741.png', 'img_01740.png', 'img_01027.png', 'img_01699.png', 'img_01022.png', 'img_02215.png', 'img_02221.png', 'img_02476.png', 'img_01608.png', 'img_01707.png', 'img_02219.png', 'img_01744.png', 'img_01748.png', 'img_01625.png', 'img_02211.png', 'img_01737.png', 'img_02024.png', 'img_01543.png', 'img_01623.png', 'img_01627.png', 'img_01559.png', 'img_01747.png', 'img_01729.png', 'img_01602.png', 'img_01689.png', 'img_02216.png', 'img_02502.png', 'img_02488.png', 'img_01618.png', 'img_02207.png', 'img_02212.png', 'img_01651.png', 'img_02483.png', 'img_02223.png', 'img_02013.png', 'img_02454.png', 'img_01612.png', 'img_02228.png', 'img_01661.png', 'img_01742.png', 'img_02026.png', 'img_02018.png', 'img_01656.png', 'img_01730.png', 'img_02229.png', 'img_01604.png', 'img_01617.png', 'img_01726.png', 'img_02498.png', 'img_02218.png', 'img_01606.png', 'img_01653.png', 'img_01654.png', 'img_01734.png', 'img_02501.png', 'img_01650.png', 'img_01603.png', 'img_01613.png', 'img_01659.png', 'img_01749.png', 'img_01026.png', 'img_01607.png', 'img_01668.png', 'img_02030.png', 'img_01743.png', 'img_01609.png', 'img_01614.png', 'img_01663.png', 'img_01649.png', 'img_01622.png', 'img_02017.png', 'img_02208.png', 'img_02205.png', 'img_02217.png', 'img_01628.png', 'img_01738.png', 'img_01701.png', 'img_01736.png', 'img_01616.png', 'img_01652.png', 'img_02019.png', 'img_01615.png', 'img_02210.png', 'img_01664.png', 'img_01666.png', 'img_01023.png', 'img_02453.png', 'img_02028.png', 'img_01667.png', 'img_02226.png', 'img_01624.png', 'img_01028.png', 'img_02227.png', 'img_01735.png', 'img_02016.png', 'img_01621.png', 'img_01658.png', 'img_01728.png', 'img_01746.png', 'img_01020.png', 'img_01727.png', 'img_01655.png', 'img_01706.png', 'img_02209.png', 'img_01029.png', 'img_02206.png', 'img_01657.png']\n",
      "Number of plant images for plant milk_thistle : 154\n",
      "['img_01774.png', 'img_01777.png', 'img_01776.png', 'img_00937.png', 'img_01773.png', 'img_01278.png', 'img_01050.png', 'img_01271.png', 'img_01149.png', 'img_00964.png', 'img_01154.png', 'img_00963.png', 'img_00967.png', 'img_01202.png', 'img_01200.png', 'img_01272.png', 'img_01775.png', 'img_01056.png', 'img_01769.png', 'img_01201.png', 'img_01052.png', 'img_01053.png', 'img_01206.png', 'img_01057.png', 'img_00968.png', 'img_01208.png', 'img_01276.png', 'img_01771.png', 'img_01151.png', 'img_00960.png', 'img_01156.png', 'img_01270.png', 'img_00965.png', 'img_01772.png', 'img_01205.png', 'img_01203.png', 'img_01274.png', 'img_01269.png', 'img_01199.png', 'img_01157.png', 'img_01770.png', 'img_01153.png', 'img_01277.png', 'img_01055.png', 'img_01059.png', 'img_01273.png', 'img_01204.png', 'img_01275.png', 'img_01058.png', 'img_01152.png', 'img_01054.png', 'img_00966.png', 'img_01158.png', 'img_01207.png', 'img_00969.png', 'img_01155.png', 'img_00961.png', 'img_00962.png', 'img_01051.png', 'img_01150.png']\n",
      "Number of plant images for plant rye_brome : 60\n",
      "['img_01804.png', 'img_00652.png', 'img_00643.png', 'img_00649.png', 'img_00650.png', 'img_01787.png', 'img_01778.png', 'img_01785.png', 'img_01806.png', 'img_00651.png', 'img_01786.png', 'img_00645.png', 'img_00646.png', 'img_00644.png', 'img_01779.png', 'img_01781.png', 'img_00647.png', 'img_01782.png', 'img_01784.png', 'img_00648.png', 'img_01783.png', 'img_01780.png']\n",
      "Number of plant images for plant narrow-leaved_plantain : 22\n",
      "['img_00348.png', 'img_00546.png', 'img_00603.png', 'img_00342.png', 'img_00536.png', 'img_00606.png', 'img_00476.png', 'img_00528.png', 'img_00736.png', 'img_00469.png', 'img_00854.png', 'img_00482.png', 'img_00479.png', 'img_00148.png', 'img_00597.png', 'img_00542.png', 'img_00345.png', 'img_00146.png', 'img_00474.png', 'img_00796.png', 'img_00601.png', 'img_00797.png', 'img_00343.png', 'img_00529.png', 'img_00600.png', 'img_00357.png', 'img_00605.png', 'img_00147.png', 'img_00351.png', 'img_00341.png', 'img_00602.png', 'img_00908.png', 'img_00346.png', 'img_00532.png', 'img_00599.png', 'img_00486.png', 'img_00737.png', 'img_00349.png', 'img_00547.png', 'img_00742.png', 'img_00145.png', 'img_00344.png', 'img_00347.png', 'img_00350.png', 'img_00914.png', 'img_00852.png', 'img_00598.png', 'img_00149.png', 'img_00477.png', 'img_00096.png', 'img_00791.png', 'img_00468.png', 'img_00604.png', 'img_00607.png', 'img_00596.png', 'img_00595.png', 'img_00545.png', 'img_00478.png', 'img_00352.png', 'img_00353.png', 'img_00356.png', 'img_00745.png', 'img_00467.png', 'img_00358.png', 'img_00355.png', 'img_00853.png', 'img_00851.png', 'img_00354.png', 'img_00480.png', 'img_00850.png', 'img_00144.png', 'img_00150.png']\n",
      "Number of plant images for plant small-flower_geranium : 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.weight', 'decode_head.batch_norm.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.bias', 'decode_head.linear_c.0.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2023-09-09 16:40:19,293] A new study created in memory with name: no-name-6163023c-3647-4489-a47c-99c00c867b56\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.weight', 'decode_head.batch_norm.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.classifier.bias', 'decode_head.linear_c.0.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 1/3900 [00:03<4:12:27,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7285, 'learning_rate': 1.023820073388892e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3900 [00:06<3:22:57,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7358, 'learning_rate': 1.0235574880917929e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3900 [00:09<3:06:22,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.736, 'learning_rate': 1.0232949027946941e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/3900 [00:11<2:58:27,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.731, 'learning_rate': 1.023032317497595e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3900 [00:14<2:54:14,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7336, 'learning_rate': 1.022769732200496e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3900 [00:16<2:51:57,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7156, 'learning_rate': 1.0225071469033971e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3900 [00:19<2:50:54,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7023, 'learning_rate': 1.0222445616062982e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3900 [00:21<2:49:31,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7067, 'learning_rate': 1.0219819763091993e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3900 [00:24<2:48:25,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7051, 'learning_rate': 1.0217193910121002e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3900 [00:27<2:47:48,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6948, 'learning_rate': 1.0214568057150012e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3900 [00:29<2:46:59,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7044, 'learning_rate': 1.0211942204179023e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3900 [00:32<2:46:45,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6609, 'learning_rate': 1.0209316351208033e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/3900 [00:34<2:46:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6824, 'learning_rate': 1.0206690498237044e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/3900 [00:37<2:46:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6774, 'learning_rate': 1.0204064645266053e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/3900 [00:39<2:46:10,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6467, 'learning_rate': 1.0201438792295063e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/3900 [00:42<2:46:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6896, 'learning_rate': 1.0198812939324074e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/3900 [00:44<2:46:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6328, 'learning_rate': 1.0196187086353085e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/3900 [00:47<2:46:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6514, 'learning_rate': 1.0193561233382095e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/3900 [00:50<2:46:34,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6619, 'learning_rate': 1.0190935380411104e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3900 [00:52<2:46:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6572, 'learning_rate': 1.0188309527440115e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/3900 [00:55<2:46:10,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6547, 'learning_rate': 1.0185683674469125e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3900 [00:57<2:45:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6441, 'learning_rate': 1.0183057821498136e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/3900 [01:00<2:45:40,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6462, 'learning_rate': 1.0180431968527147e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/3900 [01:02<2:45:31,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6591, 'learning_rate': 1.0177806115556156e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/3900 [01:05<2:45:20,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6323, 'learning_rate': 1.0175180262585166e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/3900 [01:08<2:45:31,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6265, 'learning_rate': 1.0172554409614177e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3900 [01:10<2:45:29,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5967, 'learning_rate': 1.0169928556643187e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/3900 [01:13<2:45:19,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6272, 'learning_rate': 1.0167302703672198e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/3900 [01:15<2:45:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6197, 'learning_rate': 1.0164676850701207e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/3900 [01:18<2:45:19,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5897, 'learning_rate': 1.0162050997730217e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/3900 [01:20<2:45:27,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6227, 'learning_rate': 1.0159425144759228e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 32/3900 [01:23<2:45:39,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6073, 'learning_rate': 1.0156799291788239e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/3900 [01:26<2:45:32,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5825, 'learning_rate': 1.015417343881725e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 34/3900 [01:28<2:45:19,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5822, 'learning_rate': 1.0151547585846258e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35/3900 [01:31<2:45:07,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5767, 'learning_rate': 1.0148921732875269e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 36/3900 [01:33<2:45:07,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.55, 'learning_rate': 1.014629587990428e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 37/3900 [01:36<2:44:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6104, 'learning_rate': 1.014367002693329e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 38/3900 [01:38<2:44:40,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.546, 'learning_rate': 1.0141044173962299e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 39/3900 [01:41<2:44:34,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5984, 'learning_rate': 1.013841832099131e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/3900 [01:43<2:44:25,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5944, 'learning_rate': 1.0135792468020322e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/3900 [01:46<2:44:16,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5403, 'learning_rate': 1.013316661504933e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 42/3900 [01:49<2:44:38,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4914, 'learning_rate': 1.0130540762078341e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 43/3900 [01:51<2:44:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5506, 'learning_rate': 1.012791490910735e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 44/3900 [01:54<2:44:32,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5951, 'learning_rate': 1.0125289056136363e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 45/3900 [01:56<2:44:42,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5624, 'learning_rate': 1.0122663203165373e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 46/3900 [01:59<2:44:34,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5015, 'learning_rate': 1.0120037350194382e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 47/3900 [02:01<2:44:19,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5225, 'learning_rate': 1.0117411497223393e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 48/3900 [02:04<2:44:23,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.518, 'learning_rate': 1.0114785644252403e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 49/3900 [02:07<2:44:15,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4704, 'learning_rate': 1.0112159791281414e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 50/3900 [02:09<2:44:15,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4846, 'learning_rate': 1.0109533938310424e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 51/3900 [02:12<2:44:08,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4492, 'learning_rate': 1.0106908085339433e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 52/3900 [02:14<2:44:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5109, 'learning_rate': 1.0104282232368444e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 53/3900 [02:17<2:44:01,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4915, 'learning_rate': 1.0101656379397455e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 54/3900 [02:19<2:44:04,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.458, 'learning_rate': 1.0099030526426465e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 55/3900 [02:22<2:43:49,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4828, 'learning_rate': 1.0096404673455476e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 56/3900 [02:24<2:44:02,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4695, 'learning_rate': 1.0093778820484485e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 57/3900 [02:27<2:44:04,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4987, 'learning_rate': 1.0091152967513495e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 58/3900 [02:30<2:44:07,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4364, 'learning_rate': 1.0088527114542506e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/3900 [02:32<2:43:48,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4429, 'learning_rate': 1.0085901261571517e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/3900 [02:35<2:44:02,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4619, 'learning_rate': 1.0083275408600527e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 61/3900 [02:37<2:44:04,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4738, 'learning_rate': 1.0080649555629536e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 62/3900 [02:40<2:44:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4939, 'learning_rate': 1.0078023702658547e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 63/3900 [02:42<2:44:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4424, 'learning_rate': 1.0075397849687557e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 64/3900 [02:45<2:44:29,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4398, 'learning_rate': 1.0072771996716568e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 65/3900 [02:48<2:44:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5395, 'learning_rate': 1.0070146143745577e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 66/3900 [02:50<2:44:07,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4045, 'learning_rate': 1.0067520290774587e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 67/3900 [02:53<2:43:54,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3623, 'learning_rate': 1.0064894437803598e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 68/3900 [02:55<2:44:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4309, 'learning_rate': 1.0062268584832609e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 69/3900 [02:58<2:44:15,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3988, 'learning_rate': 1.005964273186162e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 70/3900 [03:00<2:44:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4248, 'learning_rate': 1.0057016878890628e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 71/3900 [03:03<2:43:58,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4082, 'learning_rate': 1.0054391025919639e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 72/3900 [03:06<2:44:17,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3644, 'learning_rate': 1.005176517294865e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 73/3900 [03:08<2:44:17,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5203, 'learning_rate': 1.004913931997766e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 74/3900 [03:11<2:44:09,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.34, 'learning_rate': 1.004651346700667e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 75/3900 [03:13<2:43:55,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3698, 'learning_rate': 1.004388761403568e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 76/3900 [03:16<2:43:50,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3657, 'learning_rate': 1.004126176106469e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 77/3900 [03:18<2:41:48,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3717, 'learning_rate': 1.0038635908093702e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 78/3900 [03:20<2:23:51,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4084, 'learning_rate': 1.0036010055122711e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 79/3900 [03:23<2:42:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3894, 'learning_rate': 1.0033384202151722e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 80/3900 [03:26<2:42:36,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4008, 'learning_rate': 1.003075834918073e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 81/3900 [03:28<2:42:44,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3322, 'learning_rate': 1.0028132496209743e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 82/3900 [03:31<2:42:44,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4394, 'learning_rate': 1.0025506643238754e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 83/3900 [03:33<2:42:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3533, 'learning_rate': 1.0022880790267763e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 84/3900 [03:36<2:42:54,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4142, 'learning_rate': 1.0020254937296773e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 85/3900 [03:38<2:42:41,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.34, 'learning_rate': 1.0017629084325782e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 86/3900 [03:41<2:42:21,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2909, 'learning_rate': 1.0015003231354794e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 87/3900 [03:44<2:42:36,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3237, 'learning_rate': 1.0012377378383803e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 88/3900 [03:46<2:42:21,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2916, 'learning_rate': 1.0009751525412814e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 89/3900 [03:49<2:42:14,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3121, 'learning_rate': 1.0007125672441824e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 90/3900 [03:51<2:42:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2428, 'learning_rate': 1.0004499819470835e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 91/3900 [03:54<2:42:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2967, 'learning_rate': 1.0001873966499846e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 92/3900 [03:56<2:42:18,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2753, 'learning_rate': 9.999248113528855e-06, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 93/3900 [03:59<2:42:20,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2785, 'learning_rate': 9.996622260557865e-06, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 94/3900 [04:01<2:42:10,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3265, 'learning_rate': 9.993996407586876e-06, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 95/3900 [04:04<2:42:32,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2555, 'learning_rate': 9.991370554615886e-06, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 96/3900 [04:07<2:42:26,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3067, 'learning_rate': 9.988744701644897e-06, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 97/3900 [04:09<2:42:28,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2787, 'learning_rate': 9.986118848673906e-06, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 98/3900 [04:12<2:42:17,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.268, 'learning_rate': 9.983492995702917e-06, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 99/3900 [04:14<2:42:33,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2498, 'learning_rate': 9.980867142731927e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3900 [04:17<2:42:07,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2782, 'learning_rate': 9.978241289760938e-06, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-09-09 16:45:52,829] Trial 0 failed with parameters: {'learning_rate': 1.024082658685991e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/integrations/integration_utils.py\", line 199, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py\", line 1553, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py\", line 1927, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py\", line 2254, in _maybe_log_save_evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py\", line 2968, in evaluate\n",
      "    output = eval_loop(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py\", line 3261, in evaluation_loop\n",
      "    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_17457/2999676875.py\", line 7, in <lambda>\n",
      "    compute_metrics=lambda eval_pred: compute_metrics(num_labels, eval_pred),\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_17457/1524770570.py\", line 13, in compute_metrics\n",
      "    metrics = metric.compute(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/evaluate/module.py\", line 432, in compute\n",
      "    self.add_batch(**inputs)\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/evaluate/module.py\", line 486, in add_batch\n",
      "    batch = self.selected_feature_format.encode_batch(batch)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1885, in encode_batch\n",
      "    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1885, in <listcomp>\n",
      "    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1279, in encode_nested_example\n",
      "    return [encode_nested_example(schema.feature, o, level=level + 1) for o in obj]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1279, in <listcomp>\n",
      "    return [encode_nested_example(schema.feature, o, level=level + 1) for o in obj]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1279, in encode_nested_example\n",
      "    return [encode_nested_example(schema.feature, o, level=level + 1) for o in obj]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1279, in <listcomp>\n",
      "    return [encode_nested_example(schema.feature, o, level=level + 1) for o in obj]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 1284, in encode_nested_example\n",
      "    return schema.encode_example(obj) if obj is not None else None\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py\", line 512, in encode_example\n",
      "    elif pa.types.is_integer(self.pa_type):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kate/miniconda3/envs/master/lib/python3.11/site-packages/pyarrow/types.py\", line 68, in is_integer\n",
      "    def is_integer(t):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2023-09-09 16:45:52,832] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m id2label, label2id \u001b[39m=\u001b[39m get_labels(crop, model_type)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# model = model_init(id2label, label2id)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m best_trial \u001b[39m=\u001b[39m initialize_trainer_for_hp_search(\u001b[39mlen\u001b[39;49m(id2label), train_ds, val_ds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_trial\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# trainer.train()\u001b[39;00m\n",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_trainer_for_hp_search\u001b[39m(num_labels, train_ds, test_ds) :\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         model_init\u001b[39m=\u001b[39mmodel_init,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         callbacks\u001b[39m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     best_trial \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mhyperparameter_search(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         direction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mminimize\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moptuna\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         hp_space\u001b[39m=\u001b[39;49moptuna_hp_space,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:2575\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_name \u001b[39m=\u001b[39m hp_name\n\u001b[1;32m   2573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_objective \u001b[39m=\u001b[39m default_compute_objective \u001b[39mif\u001b[39;00m compute_objective \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m compute_objective\n\u001b[0;32m-> 2575\u001b[0m best_run \u001b[39m=\u001b[39m backend_obj\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m, n_trials, direction, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2577\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_search_backend \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2578\u001b[0m \u001b[39mreturn\u001b[39;00m best_run\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/hyperparameter_search.py:72\u001b[0m, in \u001b[0;36mOptunaBackend.run\u001b[0;34m(self, trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, trainer, n_trials: \u001b[39mint\u001b[39m, direction: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m run_hp_search_optuna(trainer, n_trials, direction, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:209\u001b[0m, in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m n_jobs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    208\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39mdirection, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 209\u001b[0m study\u001b[39m.\u001b[39;49moptimize(_objective, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout, n_jobs\u001b[39m=\u001b[39;49mn_jobs)\n\u001b[1;32m    210\u001b[0m best_trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n\u001b[1;32m    211\u001b[0m \u001b[39mreturn\u001b[39;00m BestRun(\u001b[39mstr\u001b[39m(best_trial\u001b[39m.\u001b[39mnumber), best_trial\u001b[39m.\u001b[39mvalue, best_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     _optimize(\n\u001b[1;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:199\u001b[0m, in \u001b[0;36mrun_hp_search_optuna.<locals>._objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    197\u001b[0m     trainer\u001b[39m.\u001b[39mtrain(resume_from_checkpoint\u001b[39m=\u001b[39mcheckpoint)\n\u001b[1;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint, trial\u001b[39m=\u001b[39;49mtrial)\n\u001b[1;32m    200\u001b[0m \u001b[39m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(trainer, \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1554\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1555\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1556\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1557\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1558\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:1927\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1925\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1927\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:2254\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2254\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2257\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:2968\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2965\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2967\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2968\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2969\u001b[0m     eval_dataloader,\n\u001b[1;32m   2970\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2971\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2972\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2973\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2974\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2975\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2976\u001b[0m )\n\u001b[1;32m   2978\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:3261\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3257\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3258\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3259\u001b[0m         )\n\u001b[1;32m   3260\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3261\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_trainer_for_hp_search\u001b[39m(num_labels, train_ds, test_ds) :\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         model_init\u001b[39m=\u001b[39mmodel_init,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         train_dataset\u001b[39m=\u001b[39mtrain_ds,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         eval_dataset\u001b[39m=\u001b[39mtest_ds,\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m eval_pred: compute_metrics(num_labels, eval_pred),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         callbacks\u001b[39m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     best_trial \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mhyperparameter_search(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptuna\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         hp_space\u001b[39m=\u001b[39moptuna_hp_space,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_trial\n",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m logits_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39minterpolate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     logits_tensor,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     size\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pred_labels \u001b[39m=\u001b[39m logits_tensor\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m metrics \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     predictions\u001b[39m=\u001b[39;49mpred_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     references\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     num_labels\u001b[39m=\u001b[39;49mnum_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     ignore_index\u001b[39m=\u001b[39;49m\u001b[39m255\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     reduce_labels\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(value) \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mndarray:\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/evaluate/module.py:486\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(column) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    485\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_nested_string_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format[key], column[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 486\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselected_feature_format\u001b[39m.\u001b[39;49mencode_batch(batch)\n\u001b[1;32m    487\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39mwrite_batch(batch)\n\u001b[1;32m    488\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid, \u001b[39mTypeError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1885\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1884\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1885\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m column]\n\u001b[1;32m   1886\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1885\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1884\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1885\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[1;32m   1886\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1279\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[39m# be careful when comparing tensors here\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1276\u001b[0m                 \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(first_elmt, \u001b[39mlist\u001b[39m)\n\u001b[1;32m   1277\u001b[0m                 \u001b[39mor\u001b[39;00m encode_nested_example(schema\u001b[39m.\u001b[39mfeature, first_elmt, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m first_elmt\n\u001b[1;32m   1278\u001b[0m             ):\n\u001b[0;32m-> 1279\u001b[0m                 \u001b[39mreturn\u001b[39;00m [encode_nested_example(schema\u001b[39m.\u001b[39;49mfeature, o, level\u001b[39m=\u001b[39;49mlevel \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m obj]\n\u001b[1;32m   1280\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n\u001b[1;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[39m# be careful when comparing tensors here\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1276\u001b[0m                 \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(first_elmt, \u001b[39mlist\u001b[39m)\n\u001b[1;32m   1277\u001b[0m                 \u001b[39mor\u001b[39;00m encode_nested_example(schema\u001b[39m.\u001b[39mfeature, first_elmt, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m first_elmt\n\u001b[1;32m   1278\u001b[0m             ):\n\u001b[0;32m-> 1279\u001b[0m                 \u001b[39mreturn\u001b[39;00m [encode_nested_example(schema\u001b[39m.\u001b[39;49mfeature, o, level\u001b[39m=\u001b[39;49mlevel \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m   1280\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n\u001b[1;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1279\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[39m# be careful when comparing tensors here\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1276\u001b[0m                 \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(first_elmt, \u001b[39mlist\u001b[39m)\n\u001b[1;32m   1277\u001b[0m                 \u001b[39mor\u001b[39;00m encode_nested_example(schema\u001b[39m.\u001b[39mfeature, first_elmt, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m first_elmt\n\u001b[1;32m   1278\u001b[0m             ):\n\u001b[0;32m-> 1279\u001b[0m                 \u001b[39mreturn\u001b[39;00m [encode_nested_example(schema\u001b[39m.\u001b[39;49mfeature, o, level\u001b[39m=\u001b[39;49mlevel \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m obj]\n\u001b[1;32m   1280\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n\u001b[1;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[39m# be careful when comparing tensors here\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1276\u001b[0m                 \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(first_elmt, \u001b[39mlist\u001b[39m)\n\u001b[1;32m   1277\u001b[0m                 \u001b[39mor\u001b[39;00m encode_nested_example(schema\u001b[39m.\u001b[39mfeature, first_elmt, level\u001b[39m=\u001b[39mlevel \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m first_elmt\n\u001b[1;32m   1278\u001b[0m             ):\n\u001b[0;32m-> 1279\u001b[0m                 \u001b[39mreturn\u001b[39;00m [encode_nested_example(schema\u001b[39m.\u001b[39;49mfeature, o, level\u001b[39m=\u001b[39;49mlevel \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m   1280\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(obj)\n\u001b[1;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:1284\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n\u001b[0;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m schema\u001b[39m.\u001b[39;49mencode_example(obj) \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/datasets/features/features.py:512\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_boolean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n\u001b[1;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(value)\n\u001b[0;32m--> 512\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39;49mtypes\u001b[39m.\u001b[39;49mis_integer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpa_type):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(value)\n\u001b[1;32m    514\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_floating(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/pyarrow/types.py:68\u001b[0m, in \u001b[0;36mis_integer\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    Return True if value is an instance of a boolean type.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m    t : DataType\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mid \u001b[39m==\u001b[39m lib\u001b[39m.\u001b[39mType_BOOL\n\u001b[0;32m---> 68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_integer\u001b[39m(t):\n\u001b[1;32m     69\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    Return True if value is an instance of any integer type.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m    t : DataType\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mid \u001b[39min\u001b[39;00m _INTEGER_TYPES\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_type = \"multiclass\"\n",
    "crop = \"broad_bean\" \n",
    "model_plant_names = [crop] + weed_plants\n",
    "train_ds, val_ds, test_ds = create_datasets_for_plants(model_plant_names, model_type, crop)\n",
    "\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(train_transforms)\n",
    "\n",
    "id2label, label2id = get_labels(crop, model_type)\n",
    "\n",
    "# model = model_init(id2label, label2id)\n",
    "best_trial = initialize_trainer_for_hp_search(len(id2label), train_ds, val_ds)\n",
    "best_trial\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_of_type_for_crop(model_type, crop):\n",
    "    model_plant_names = [crop] + weed_plants\n",
    "    train_ds, val_ds, test_ds = create_datasets_for_plants(model_plant_names, model_type, crop)\n",
    "\n",
    "    print(\"Training subset number of images: \" + str(train_ds.num_rows))\n",
    "    print(\"Validation subset number of images: \" + str(val_ds.num_rows))\n",
    "    print(\"Test subset number of images: \" + str(test_ds.num_rows))\n",
    "\n",
    "    train_ds.set_transform(train_transforms)\n",
    "    val_ds.set_transform(train_transforms)\n",
    "    test_ds.set_transform(train_transforms)\n",
    "\n",
    "    id2label, label2id = get_labels(crop, model_type)\n",
    "\n",
    "    print('Number of classes:', len(id2label))\n",
    "    print('id2label:', id2label)\n",
    "    print('label2id:', label2id)\n",
    "\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n",
    "    trainer = initialize_trainer(model, len(id2label), train_ds, val_ds)\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the trained model, so that it can be used for inference later.\n",
    "    # Save the log history, so that it can be used for plotting later.\n",
    "    trainer.save_model('models/' + model_type + '/' + crop)\n",
    "    with open('models/' + model_type + '/' + crop + '/log_history.json', 'w') as file:\n",
    "        log_history = trainer.state.log_history\n",
    "        json.dump(log_history, file)\n",
    "\n",
    "    test_metric = trainer.evaluate(test_ds)\n",
    "    test_metric\n",
    "\n",
    "    with open('models/' + model_type + '/' + crop + '/test_metric.json', 'w') as file:\n",
    "        json.dump(test_metric, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_00071.png', 'img_00072.png', 'img_00074.png', 'img_00083.png', 'img_00084.png', 'img_00085.png', 'img_00151.png', 'img_00152.png', 'img_00153.png', 'img_00154.png', 'img_00155.png', 'img_00156.png', 'img_00210.png', 'img_00211.png', 'img_00212.png', 'img_00213.png', 'img_00214.png', 'img_00215.png', 'img_00216.png', 'img_00217.png', 'img_00218.png', 'img_00220.png', 'img_00221.png', 'img_00226.png', 'img_00227.png', 'img_00229.png', 'img_00248.png', 'img_00249.png', 'img_00250.png', 'img_00251.png', 'img_00254.png', 'img_00255.png', 'img_00256.png', 'img_00257.png', 'img_00258.png', 'img_00259.png', 'img_00260.png', 'img_00264.png', 'img_00265.png', 'img_00266.png', 'img_00267.png', 'img_00268.png', 'img_00269.png', 'img_00271.png', 'img_00293.png', 'img_00294.png', 'img_00295.png', 'img_00296.png', 'img_00297.png', 'img_00298.png', 'img_00299.png', 'img_00300.png', 'img_00301.png', 'img_00302.png', 'img_00303.png', 'img_00379.png', 'img_00380.png', 'img_00381.png', 'img_00382.png', 'img_00383.png', 'img_00384.png', 'img_00385.png', 'img_00386.png', 'img_00387.png', 'img_00388.png', 'img_00389.png', 'img_00390.png', 'img_00391.png', 'img_00436.png', 'img_00437.png', 'img_00438.png', 'img_00439.png', 'img_00441.png', 'img_00442.png', 'img_00443.png', 'img_00444.png', 'img_00445.png', 'img_00446.png', 'img_00453.png', 'img_00455.png', 'img_00458.png', 'img_00459.png', 'img_00460.png', 'img_00461.png', 'img_00462.png', 'img_00463.png', 'img_00464.png', 'img_00489.png', 'img_00490.png', 'img_00491.png', 'img_00495.png', 'img_00496.png', 'img_00497.png', 'img_00498.png', 'img_00499.png', 'img_00500.png', 'img_00502.png', 'img_00525.png', 'img_00526.png', 'img_00527.png', 'img_00530.png', 'img_00531.png', 'img_00533.png', 'img_00534.png', 'img_00535.png', 'img_00537.png', 'img_00538.png', 'img_00539.png', 'img_00540.png', 'img_00541.png', 'img_00543.png', 'img_00544.png', 'img_00583.png', 'img_00584.png', 'img_00585.png', 'img_00586.png', 'img_00587.png', 'img_00588.png', 'img_00589.png', 'img_00590.png', 'img_00591.png', 'img_00592.png', 'img_00593.png', 'img_00660.png', 'img_00661.png', 'img_00662.png', 'img_00663.png', 'img_00664.png', 'img_00665.png', 'img_00666.png', 'img_00667.png', 'img_00668.png', 'img_00669.png', 'img_00670.png', 'img_00671.png', 'img_00718.png', 'img_00719.png', 'img_00720.png', 'img_00721.png', 'img_00722.png', 'img_00723.png', 'img_00724.png', 'img_00725.png', 'img_00727.png', 'img_00728.png', 'img_00730.png', 'img_00731.png', 'img_00732.png', 'img_00733.png', 'img_00734.png', 'img_00735.png', 'img_00746.png', 'img_00747.png', 'img_00748.png', 'img_00749.png', 'img_00751.png', 'img_00754.png', 'img_00757.png', 'img_00760.png', 'img_00761.png', 'img_00762.png', 'img_00763.png', 'img_00764.png', 'img_00765.png', 'img_00766.png', 'img_00767.png', 'img_00769.png', 'img_00789.png', 'img_00790.png', 'img_00792.png', 'img_00793.png', 'img_00794.png', 'img_00795.png', 'img_00798.png', 'img_00799.png', 'img_00800.png', 'img_00801.png', 'img_00802.png', 'img_00835.png', 'img_00836.png', 'img_00837.png', 'img_00839.png', 'img_00840.png', 'img_00842.png', 'img_00843.png', 'img_00844.png', 'img_00846.png', 'img_00874.png', 'img_00875.png', 'img_00876.png', 'img_00877.png', 'img_00878.png', 'img_00879.png', 'img_00880.png', 'img_00881.png', 'img_00901.png', 'img_00902.png', 'img_00903.png', 'img_00904.png', 'img_00905.png', 'img_00906.png', 'img_00915.png', 'img_00916.png', 'img_00917.png', 'img_00919.png', 'img_00920.png', 'img_00921.png', 'img_00922.png', 'img_00923.png', 'img_00924.png', 'img_00970.png', 'img_00971.png', 'img_00972.png', 'img_00973.png', 'img_00974.png', 'img_00975.png', 'img_00976.png', 'img_00977.png', 'img_00978.png', 'img_00979.png', 'img_01060.png', 'img_01061.png', 'img_01062.png', 'img_01063.png', 'img_01064.png', 'img_01065.png', 'img_01066.png', 'img_01067.png', 'img_01068.png', 'img_01069.png', 'img_01110.png', 'img_01111.png', 'img_01112.png', 'img_01113.png', 'img_01114.png', 'img_01115.png', 'img_01116.png', 'img_01117.png', 'img_01118.png', 'img_01119.png', 'img_01159.png', 'img_01160.png', 'img_01161.png', 'img_01162.png', 'img_01163.png', 'img_01164.png', 'img_01165.png', 'img_01166.png', 'img_01167.png', 'img_01168.png', 'img_01209.png', 'img_01210.png', 'img_01211.png', 'img_01212.png', 'img_01213.png', 'img_01214.png', 'img_01215.png', 'img_01216.png', 'img_01217.png', 'img_01218.png', 'img_01239.png', 'img_01240.png', 'img_01241.png', 'img_01242.png', 'img_01244.png', 'img_01245.png', 'img_01247.png', 'img_01248.png', 'img_01289.png', 'img_01290.png', 'img_01291.png', 'img_01292.png', 'img_01293.png', 'img_01294.png', 'img_01295.png', 'img_01296.png', 'img_01297.png', 'img_01298.png', 'img_01378.png', 'img_01379.png', 'img_01380.png', 'img_01381.png', 'img_01382.png', 'img_01383.png', 'img_01384.png', 'img_01385.png', 'img_01386.png', 'img_01387.png', 'img_01388.png', 'img_01389.png', 'img_01390.png', 'img_01391.png', 'img_01392.png', 'img_01393.png', 'img_01394.png', 'img_01395.png', 'img_01816.png', 'img_01817.png', 'img_01818.png', 'img_01819.png', 'img_01820.png', 'img_01865.png', 'img_01866.png', 'img_01867.png', 'img_01868.png', 'img_01869.png', 'img_01870.png', 'img_02068.png', 'img_02069.png', 'img_02070.png', 'img_02071.png', 'img_02072.png', 'img_02073.png', 'img_02074.png', 'img_02256.png', 'img_02257.png', 'img_02258.png', 'img_02259.png', 'img_02260.png', 'img_02261.png', 'img_02262.png', 'img_02263.png', 'img_02264.png', 'img_02265.png', 'img_02266.png', 'img_02267.png', 'img_02268.png', 'img_02269.png', 'img_02270.png', 'img_02271.png', 'img_02272.png', 'img_02273.png', 'img_02274.png', 'img_02275.png', 'img_02276.png', 'img_02277.png', 'img_02278.png', 'img_02279.png', 'img_02280.png', 'img_02281.png', 'img_02282.png', 'img_02283.png', 'img_02284.png', 'img_02285.png', 'img_02286.png', 'img_02287.png', 'img_02288.png', 'img_02289.png', 'img_02290.png', 'img_02291.png', 'img_02292.png', 'img_02293.png', 'img_02294.png', 'img_02446.png', 'img_02447.png', 'img_02448.png', 'img_02449.png', 'img_02450.png', 'img_02451.png', 'img_02452.png', 'img_02455.png', 'img_02456.png', 'img_02457.png', 'img_02458.png', 'img_02459.png', 'img_02460.png', 'img_02461.png', 'img_02462.png', 'img_02463.png', 'img_02464.png', 'img_02465.png', 'img_02466.png', 'img_02467.png', 'img_02468.png', 'img_02469.png', 'img_02471.png', 'img_02472.png', 'img_02473.png', 'img_02474.png', 'img_02475.png', 'img_02478.png', 'img_02479.png', 'img_02480.png', 'img_02481.png', 'img_02482.png', 'img_02484.png', 'img_02485.png', 'img_02486.png', 'img_02487.png', 'img_02503.png', 'img_02504.png', 'img_02505.png', 'img_02506.png', 'img_02507.png', 'img_02508.png', 'img_02509.png', 'img_02510.png', 'img_02511.png', 'img_02512.png', 'img_02513.png', 'img_02514.png', 'img_02515.png', 'img_02516.png', 'img_02517.png', 'img_02555.png', 'img_02556.png', 'img_02557.png', 'img_02558.png', 'img_02559.png', 'img_02560.png']\n",
      "Number of plant images for plant sugar_beet : 410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_01804.png', 'img_01807.png', 'img_01790.png', 'img_01803.png', 'img_01793.png', 'img_01797.png', 'img_01798.png', 'img_01796.png', 'img_01799.png', 'img_01795.png', 'img_01791.png', 'img_01802.png', 'img_01806.png', 'img_01789.png', 'img_01805.png', 'img_01800.png', 'img_01794.png', 'img_01801.png', 'img_01792.png', 'img_01788.png']\n",
      "Number of plant images for plant corn_spurry : 20\n",
      "['img_01768.png', 'img_01767.png', 'img_01796.png']\n",
      "Number of plant images for plant red-root_amaranth : 3\n",
      "['img_01812.png', 'img_01809.png', 'img_01815.png', 'img_01810.png', 'img_01808.png', 'img_01813.png', 'img_01814.png', 'img_01811.png']\n",
      "Number of plant images for plant red_fingergrass : 8\n",
      "['img_00434.png', 'img_00203.png', 'img_00424.png', 'img_00717.png', 'img_00657.png', 'img_00430.png', 'img_00715.png', 'img_00420.png', 'img_00658.png', 'img_00209.png', 'img_00659.png', 'img_00419.png', 'img_00709.png', 'img_00713.png', 'img_00206.png', 'img_00429.png', 'img_00712.png', 'img_00421.png', 'img_00710.png', 'img_00204.png', 'img_00431.png', 'img_00435.png', 'img_00208.png', 'img_00714.png', 'img_00716.png']\n",
      "Number of plant images for plant common_wild_oat : 25\n",
      "['img_00618.png', 'img_00078.png', 'img_00612.png', 'img_01764.png', 'img_01195.png', 'img_01271.png', 'img_00447.png', 'img_01762.png', 'img_00856.png', 'img_00613.png', 'img_00223.png', 'img_01198.png', 'img_01758.png', 'img_00373.png', 'img_01189.png', 'img_00614.png', 'img_01264.png', 'img_01192.png', 'img_01266.png', 'img_01270.png', 'img_00804.png', 'img_00569.png', 'img_01753.png', 'img_00374.png', 'img_00959.png', 'img_00364.png', 'img_01751.png', 'img_00620.png', 'img_01048.png', 'img_00857.png', 'img_00936.png', 'img_01196.png', 'img_01191.png', 'img_01143.png', 'img_00810.png', 'img_01752.png', 'img_00359.png', 'img_01871.png', 'img_00558.png', 'img_01263.png', 'img_00376.png', 'img_00450.png', 'img_00122.png', 'img_00858.png', 'img_00362.png', 'img_01194.png', 'img_00952.png', 'img_00378.png', 'img_01262.png', 'img_01755.png', 'img_00076.png', 'img_00954.png', 'img_00617.png', 'img_00371.png', 'img_00609.png', 'img_00118.png', 'img_00950.png', 'img_01197.png', 'img_00228.png', 'img_01754.png', 'img_00957.png', 'img_00457.png', 'img_01042.png', 'img_00466.png', 'img_00821.png', 'img_00465.png', 'img_00956.png', 'img_00073.png', 'img_00219.png', 'img_00556.png', 'img_00372.png', 'img_00224.png', 'img_01193.png', 'img_00367.png', 'img_01767.png', 'img_00608.png', 'img_01147.png', 'img_00079.png', 'img_00081.png', 'img_00368.png', 'img_01044.png', 'img_01259.png', 'img_00365.png', 'img_00082.png', 'img_00069.png', 'img_00859.png', 'img_01260.png', 'img_01190.png', 'img_01268.png', 'img_00448.png', 'img_01045.png', 'img_00440.png', 'img_00363.png', 'img_00369.png', 'img_00726.png', 'img_00452.png', 'img_00955.png', 'img_01148.png', 'img_00555.png', 'img_00812.png', 'img_01040.png', 'img_00370.png', 'img_00953.png', 'img_00222.png', 'img_01146.png', 'img_00123.png', 'img_01763.png', 'img_00075.png', 'img_00068.png', 'img_00449.png', 'img_01765.png', 'img_01766.png', 'img_00451.png', 'img_00907.png', 'img_01768.png', 'img_00456.png', 'img_01142.png', 'img_00377.png', 'img_00619.png', 'img_00611.png', 'img_00077.png', 'img_01759.png', 'img_01761.png', 'img_00360.png', 'img_01041.png', 'img_00086.png', 'img_01750.png', 'img_00951.png', 'img_01141.png', 'img_00561.png', 'img_00375.png', 'img_00615.png', 'img_00855.png', 'img_01046.png', 'img_01043.png', 'img_01757.png', 'img_00121.png', 'img_01049.png', 'img_01873.png', 'img_00454.png', 'img_00958.png', 'img_01756.png', 'img_01265.png', 'img_01145.png', 'img_01140.png', 'img_01760.png', 'img_01047.png', 'img_01267.png', 'img_00361.png', 'img_01877.png', 'img_00366.png', 'img_00811.png', 'img_00080.png', 'img_01261.png', 'img_00729.png', 'img_00070.png', 'img_00116.png', 'img_00567.png', 'img_00610.png', 'img_00616.png', 'img_00225.png', 'img_01144.png']\n",
      "Number of plant images for plant cornflower : 162\n",
      "['img_01993.png', 'img_00753.png', 'img_01719.png', 'img_01714.png', 'img_01348.png', 'img_01724.png', 'img_01128.png', 'img_02000.png', 'img_02001.png', 'img_00622.png', 'img_00636.png', 'img_01129.png', 'img_01104.png', 'img_01123.png', 'img_01109.png', 'img_01593.png', 'img_00759.png', 'img_01636.png', 'img_02194.png', 'img_00621.png', 'img_01633.png', 'img_01126.png', 'img_01249.png', 'img_01718.png', 'img_01251.png', 'img_01709.png', 'img_01349.png', 'img_01599.png', 'img_02490.png', 'img_02008.png', 'img_01256.png', 'img_01998.png', 'img_01175.png', 'img_02009.png', 'img_02492.png', 'img_00634.png', 'img_01721.png', 'img_01990.png', 'img_01997.png', 'img_02198.png', 'img_02495.png', 'img_01648.png', 'img_02193.png', 'img_01629.png', 'img_00627.png', 'img_01601.png', 'img_01992.png', 'img_01170.png', 'img_02012.png', 'img_00628.png', 'img_01580.png', 'img_01598.png', 'img_01177.png', 'img_00755.png', 'img_02203.png', 'img_01634.png', 'img_01632.png', 'img_02192.png', 'img_01005.png', 'img_02010.png', 'img_00626.png', 'img_01644.png', 'img_02005.png', 'img_01725.png', 'img_00639.png', 'img_00861.png', 'img_02190.png', 'img_00494.png', 'img_00863.png', 'img_01642.png', 'img_01647.png', 'img_02204.png', 'img_02202.png', 'img_01101.png', 'img_01720.png', 'img_01995.png', 'img_01646.png', 'img_00501.png', 'img_00270.png', 'img_01001.png', 'img_02011.png', 'img_01176.png', 'img_01246.png', 'img_00862.png', 'img_01102.png', 'img_01105.png', 'img_02195.png', 'img_02494.png', 'img_01108.png', 'img_01171.png', 'img_01637.png', 'img_00630.png', 'img_01178.png', 'img_02002.png', 'img_00631.png', 'img_01173.png', 'img_00272.png', 'img_01172.png', 'img_00262.png', 'img_00632.png', 'img_00758.png', 'img_00768.png', 'img_01258.png', 'img_01641.png', 'img_00866.png', 'img_01354.png', 'img_00624.png', 'img_00635.png', 'img_01723.png', 'img_02493.png', 'img_01125.png', 'img_01645.png', 'img_01711.png', 'img_02191.png', 'img_01722.png', 'img_00770.png', 'img_00918.png', 'img_02003.png', 'img_00864.png', 'img_01994.png', 'img_01999.png', 'img_00867.png', 'img_01004.png', 'img_01107.png', 'img_01120.png', 'img_01252.png', 'img_01631.png', 'img_01006.png', 'img_01643.png', 'img_00629.png', 'img_00637.png', 'img_01638.png', 'img_01121.png', 'img_01710.png', 'img_02491.png', 'img_01124.png', 'img_01174.png', 'img_01639.png', 'img_01003.png', 'img_00503.png', 'img_01570.png', 'img_01715.png', 'img_00633.png', 'img_01253.png', 'img_00623.png', 'img_01356.png', 'img_01103.png', 'img_01000.png', 'img_00493.png', 'img_01002.png', 'img_02196.png', 'img_00756.png', 'img_01008.png', 'img_01712.png', 'img_01169.png', 'img_01243.png', 'img_01009.png', 'img_01122.png', 'img_02004.png', 'img_00261.png', 'img_00865.png', 'img_01713.png', 'img_01250.png', 'img_01592.png', 'img_02006.png', 'img_02201.png', 'img_01257.png', 'img_01127.png', 'img_02197.png', 'img_01989.png', 'img_01594.png', 'img_02489.png', 'img_01106.png', 'img_01254.png', 'img_01630.png', 'img_01596.png', 'img_00860.png', 'img_01640.png', 'img_01635.png', 'img_00638.png', 'img_00263.png', 'img_01717.png', 'img_00625.png', 'img_01255.png', 'img_00252.png', 'img_00253.png', 'img_01597.png', 'img_00750.png', 'img_01716.png', 'img_00492.png', 'img_01595.png', 'img_00752.png', 'img_01007.png', 'img_01100.png', 'img_01600.png', 'img_01996.png', 'img_02199.png', 'img_02007.png', 'img_02200.png', 'img_01991.png']\n",
      "Number of plant images for plant corn_cockle : 200\n",
      "['img_02477.png', 'img_02225.png', 'img_01702.png', 'img_01605.png', 'img_02214.png', 'img_02499.png', 'img_01619.png', 'img_01626.png', 'img_01858.png', 'img_01662.png', 'img_01682.png', 'img_02025.png', 'img_02213.png', 'img_01024.png', 'img_01745.png', 'img_01856.png', 'img_02470.png', 'img_01610.png', 'img_01021.png', 'img_02020.png', 'img_01733.png', 'img_01665.png', 'img_01660.png', 'img_02022.png', 'img_01863.png', 'img_02029.png', 'img_01670.png', 'img_01611.png', 'img_02224.png', 'img_01620.png', 'img_02015.png', 'img_02014.png', 'img_02027.png', 'img_01861.png', 'img_02021.png', 'img_01537.png', 'img_02496.png', 'img_01739.png', 'img_02497.png', 'img_02222.png', 'img_01669.png', 'img_02220.png', 'img_01731.png', 'img_02023.png', 'img_02500.png', 'img_01732.png', 'img_01025.png', 'img_01741.png', 'img_01740.png', 'img_01027.png', 'img_01699.png', 'img_01022.png', 'img_02215.png', 'img_02221.png', 'img_02476.png', 'img_01608.png', 'img_01707.png', 'img_02219.png', 'img_01744.png', 'img_01748.png', 'img_01625.png', 'img_02211.png', 'img_01737.png', 'img_02024.png', 'img_01543.png', 'img_01623.png', 'img_01627.png', 'img_01559.png', 'img_01747.png', 'img_01729.png', 'img_01602.png', 'img_01689.png', 'img_02216.png', 'img_02502.png', 'img_02488.png', 'img_01618.png', 'img_02207.png', 'img_02212.png', 'img_01651.png', 'img_02483.png', 'img_02223.png', 'img_02013.png', 'img_02454.png', 'img_01612.png', 'img_02228.png', 'img_01661.png', 'img_01742.png', 'img_02026.png', 'img_02018.png', 'img_01656.png', 'img_01730.png', 'img_02229.png', 'img_01604.png', 'img_01617.png', 'img_01726.png', 'img_02498.png', 'img_02218.png', 'img_01606.png', 'img_01653.png', 'img_01654.png', 'img_01734.png', 'img_02501.png', 'img_01650.png', 'img_01603.png', 'img_01613.png', 'img_01659.png', 'img_01749.png', 'img_01026.png', 'img_01607.png', 'img_01668.png', 'img_02030.png', 'img_01743.png', 'img_01609.png', 'img_01614.png', 'img_01663.png', 'img_01649.png', 'img_01622.png', 'img_02017.png', 'img_02208.png', 'img_02205.png', 'img_02217.png', 'img_01628.png', 'img_01738.png', 'img_01701.png', 'img_01736.png', 'img_01616.png', 'img_01652.png', 'img_02019.png', 'img_01615.png', 'img_02210.png', 'img_01664.png', 'img_01666.png', 'img_01023.png', 'img_02453.png', 'img_02028.png', 'img_01667.png', 'img_02226.png', 'img_01624.png', 'img_01028.png', 'img_02227.png', 'img_01735.png', 'img_02016.png', 'img_01621.png', 'img_01658.png', 'img_01728.png', 'img_01746.png', 'img_01020.png', 'img_01727.png', 'img_01655.png', 'img_01706.png', 'img_02209.png', 'img_01029.png', 'img_02206.png', 'img_01657.png']\n",
      "Number of plant images for plant milk_thistle : 154\n",
      "['img_01774.png', 'img_01777.png', 'img_01776.png', 'img_00937.png', 'img_01773.png', 'img_01278.png', 'img_01050.png', 'img_01271.png', 'img_01149.png', 'img_00964.png', 'img_01154.png', 'img_00963.png', 'img_00967.png', 'img_01202.png', 'img_01200.png', 'img_01272.png', 'img_01775.png', 'img_01056.png', 'img_01769.png', 'img_01201.png', 'img_01052.png', 'img_01053.png', 'img_01206.png', 'img_01057.png', 'img_00968.png', 'img_01208.png', 'img_01276.png', 'img_01771.png', 'img_01151.png', 'img_00960.png', 'img_01156.png', 'img_01270.png', 'img_00965.png', 'img_01772.png', 'img_01205.png', 'img_01203.png', 'img_01274.png', 'img_01269.png', 'img_01199.png', 'img_01157.png', 'img_01770.png', 'img_01153.png', 'img_01277.png', 'img_01055.png', 'img_01059.png', 'img_01273.png', 'img_01204.png', 'img_01275.png', 'img_01058.png', 'img_01152.png', 'img_01054.png', 'img_00966.png', 'img_01158.png', 'img_01207.png', 'img_00969.png', 'img_01155.png', 'img_00961.png', 'img_00962.png', 'img_01051.png', 'img_01150.png']\n",
      "Number of plant images for plant rye_brome : 60\n",
      "['img_01804.png', 'img_00652.png', 'img_00643.png', 'img_00649.png', 'img_00650.png', 'img_01787.png', 'img_01778.png', 'img_01785.png', 'img_01806.png', 'img_00651.png', 'img_01786.png', 'img_00645.png', 'img_00646.png', 'img_00644.png', 'img_01779.png', 'img_01781.png', 'img_00647.png', 'img_01782.png', 'img_01784.png', 'img_00648.png', 'img_01783.png', 'img_01780.png']\n",
      "Number of plant images for plant narrow-leaved_plantain : 22\n",
      "['img_00348.png', 'img_00546.png', 'img_00603.png', 'img_00342.png', 'img_00536.png', 'img_00606.png', 'img_00476.png', 'img_00528.png', 'img_00736.png', 'img_00469.png', 'img_00854.png', 'img_00482.png', 'img_00479.png', 'img_00148.png', 'img_00597.png', 'img_00542.png', 'img_00345.png', 'img_00146.png', 'img_00474.png', 'img_00796.png', 'img_00601.png', 'img_00797.png', 'img_00343.png', 'img_00529.png', 'img_00600.png', 'img_00357.png', 'img_00605.png', 'img_00147.png', 'img_00351.png', 'img_00341.png', 'img_00602.png', 'img_00908.png', 'img_00346.png', 'img_00532.png', 'img_00599.png', 'img_00486.png', 'img_00737.png', 'img_00349.png', 'img_00547.png', 'img_00742.png', 'img_00145.png', 'img_00344.png', 'img_00347.png', 'img_00350.png', 'img_00914.png', 'img_00852.png', 'img_00598.png', 'img_00149.png', 'img_00477.png', 'img_00096.png', 'img_00791.png', 'img_00468.png', 'img_00604.png', 'img_00607.png', 'img_00596.png', 'img_00595.png', 'img_00545.png', 'img_00478.png', 'img_00352.png', 'img_00353.png', 'img_00356.png', 'img_00745.png', 'img_00467.png', 'img_00358.png', 'img_00355.png', 'img_00853.png', 'img_00851.png', 'img_00354.png', 'img_00480.png', 'img_00850.png', 'img_00144.png', 'img_00150.png']\n",
      "Number of plant images for plant small-flower_geranium : 72\n",
      "Training subset number of images: 567\n",
      "Validation subset number of images: 282\n",
      "Test subset number of images: 287\n",
      "Number of classes: 13\n",
      "id2label: {0: 'void', 1: 'soil', 2: 'sugar_beet', 3: 'corn_spurry', 4: 'red-root_amaranth', 5: 'red_fingergrass', 6: 'common_wild_oat', 7: 'cornflower', 8: 'corn_cockle', 9: 'milk_thistle', 10: 'rye_brome', 11: 'narrow-leaved_plantain', 12: 'small-flower_geranium'}\n",
      "label2id: {'void': 0, 'soil': 1, 'sugar_beet': 2, 'corn_spurry': 3, 'red-root_amaranth': 4, 'red_fingergrass': 5, 'common_wild_oat': 6, 'cornflower': 7, 'corn_cockle': 8, 'milk_thistle': 9, 'rye_brome': 10, 'narrow-leaved_plantain': 11, 'small-flower_geranium': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_var', 'decode_head.classifier.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.classifier.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "To use hyperparameter search, you need to pass your model through a model_init function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train_model_of_type_for_crop(\"multiclass\", \"broad_bean\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_model_of_type_for_crop(\u001b[39m\"\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msugar_beet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlabel2id:\u001b[39m\u001b[39m'\u001b[39m, label2id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSemanticSegmentation\u001b[39m.\u001b[39mfrom_pretrained(checkpoint, id2label\u001b[39m=\u001b[39mid2label, label2id\u001b[39m=\u001b[39mlabel2id)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m initialize_trainer(model, \u001b[39mlen\u001b[39;49m(id2label), train_ds, val_ds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Save the trained model, so that it can be used for inference later.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Save the log history, so that it can be used for plotting later.\u001b[39;00m\n",
      "\u001b[1;32m/home/kate/dev/master_thesis/03_segformer.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_trainer\u001b[39m(model, num_labels, train_ds, test_ds) :\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         callbacks\u001b[39m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     best_trial \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mhyperparameter_search(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         direction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mminimize\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msigopt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         hp_space\u001b[39m=\u001b[39;49msigopt_hp_space,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kate/dev/master_thesis/03_segformer.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_trial, trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/master/lib/python3.11/site-packages/transformers/trainer.py:2567\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_search_backend \u001b[39m=\u001b[39m backend\n\u001b[1;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2567\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2568\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use hyperparameter search, you need to pass your model through a model_init function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2569\u001b[0m     )\n\u001b[1;32m   2571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_space \u001b[39m=\u001b[39m backend_obj\u001b[39m.\u001b[39mdefault_hp_space \u001b[39mif\u001b[39;00m hp_space \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m hp_space\n\u001b[1;32m   2572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_name \u001b[39m=\u001b[39m hp_name\n",
      "\u001b[0;31mRuntimeError\u001b[0m: To use hyperparameter search, you need to pass your model through a model_init function."
     ]
    }
   ],
   "source": [
    "train_model_of_type_for_crop(\"multiclass\", \"broad_bean\")\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"sugar_beet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from typing import NoReturn\n",
    "\n",
    "# def shutdown_windows() -> NoReturn:\n",
    "#     subprocess.run([\"shutdown\", \"/s\", \"/t\", \"0\"])\n",
    "\n",
    "# shutdown_windows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
