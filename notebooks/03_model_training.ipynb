{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if GPU is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "    print('Memory Usage:') \n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, methods and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\master\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "from os import sys\n",
    "\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation, TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_prepossessing import create_datasets_for_plants, get_labels\n",
    "from constants import seed, weed_plants, models_folder\n",
    "from config import model_type, crop, checkpoint, batch_size\n",
    "from model_training import train_model_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a semantic segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_image_processor(checkpoint):\n",
    "    image_processor = SegformerImageProcessor.from_pretrained(checkpoint)\n",
    "    return image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch, image_processor):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(num_labels, metric, eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training_arguments(prediction_loss_only):\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"segformer-b0-scene-parse-150\",\n",
    "        learning_rate=6e-5,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=30,\n",
    "        eval_steps=30,\n",
    "        logging_steps=1,\n",
    "        prediction_loss_only=prediction_loss_only,\n",
    "        remove_unused_columns=False,\n",
    "        load_best_model_at_end=True,\n",
    "        seed=seed,\n",
    "        dataloader_drop_last=True\n",
    "    )\n",
    "\n",
    "\n",
    "def init_training_arguments_for_training():\n",
    "    return init_training_arguments(True)\n",
    "\n",
    "\n",
    "def init_training_arguments_for_evaluation():\n",
    "    return init_training_arguments(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer(model, training_args, num_labels, metric, train_ds, test_ds):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(num_labels, metric, eval_pred),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_of_type_for_crop(model_type, crop):\n",
    "    # Prepare the data for the model training\n",
    "    model_plant_names = [crop] + weed_plants\n",
    "    train_ds, test_ds = create_datasets_for_plants(model_plant_names, model_type, crop)\n",
    "\n",
    "    print(\"Training subset number of images: \" + str(train_ds.num_rows))\n",
    "    print(\"Test subset number of images: \" + str(test_ds.num_rows))\n",
    "\n",
    "    image_processor = init_image_processor(checkpoint)\n",
    "    train_ds.set_transform(lambda example_batch: train_transforms(example_batch, image_processor))\n",
    "    test_ds.set_transform(lambda example_batch: train_transforms(example_batch, image_processor))\n",
    "\n",
    "    # Generate labels for the model\n",
    "    id2label, label2id = get_labels(crop, model_type)\n",
    "    num_classses = len(id2label)\n",
    "\n",
    "    print('Number of classes:', num_classses)\n",
    "    print('id2label:', id2label)\n",
    "    print('label2id:', label2id)\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "    training_args_for_training = init_training_arguments_for_training()\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "    trainer = initialize_trainer(model, training_args_for_training, num_classses, metric, train_ds, test_ds)\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the trained model, so that it can be used for inference later\n",
    "    trainer.save_model(models_folder + model_type + '/' + crop)\n",
    "    \n",
    "    # Save the log history, so that it can be used for plotting later\n",
    "    with open(models_folder + model_type + '/' + crop + '/log_history.json', 'w') as file:\n",
    "        log_history = trainer.state.lo0g_history\n",
    "        json.dump(log_history, file)\n",
    "\n",
    "    # Instantiate new trainer for evaluation that will use compute_metrics method\n",
    "    training_args_for_evaluation = init_training_arguments_for_evaluation()\n",
    "    eval_trainer = initialize_trainer(trainer.model, training_args_for_evaluation, num_classses, metric, train_ds, test_ds)\n",
    "    test_metric = eval_trainer.evaluate(test_ds)\n",
    "    with open(models_folder + model_type + '/' + crop + '/test_metric.json', 'w') as file:\n",
    "        json.dump(test_metric, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_config():\n",
    "    train_model_of_type_for_crop(model_type, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset number of images: 391\n",
      "Test subset number of images: 392\n",
      "Number of classes: 4\n",
      "id2label: {0: 'void', 1: 'soil', 2: 'common_buckwheat', 3: 'weeds'}\n",
      "label2id: {'void': 0, 'soil': 1, 'common_buckwheat': 2, 'weeds': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UperNetForSemanticSegmentation were not initialized from the model checkpoint at openmmlab/upernet-swin-tiny and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 512, 1, 1]) in the checkpoint and torch.Size([4, 512, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- auxiliary_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([4, 256, 1, 1]) in the model instantiated\n",
      "- auxiliary_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# train_model_from_config()\n",
    "model_plant_names = [crop] + weed_plants\n",
    "train_ds, test_ds = create_datasets_for_plants(model_plant_names, model_type, crop)\n",
    "\n",
    "print(\"Training subset number of images: \" + str(train_ds.num_rows))\n",
    "print(\"Test subset number of images: \" + str(test_ds.num_rows))\n",
    "\n",
    "image_processor = init_image_processor(checkpoint)\n",
    "train_ds.set_transform(lambda example_batch: train_transforms(example_batch, image_processor))\n",
    "test_ds.set_transform(lambda example_batch: train_transforms(example_batch, image_processor))\n",
    "\n",
    "# Generate labels for the model\n",
    "id2label, label2id = get_labels(crop, model_type)\n",
    "num_classses = len(id2label)\n",
    "\n",
    "print('Number of classes:', num_classses)\n",
    "print('id2label:', id2label)\n",
    "print('label2id:', label2id)\n",
    "\n",
    "# Initialize and train model\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "training_args_for_training = init_training_arguments_for_training()\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "trainer = initialize_trainer(model, training_args_for_training, num_classses, metric, train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[-0.9877, -1.0048, -1.0562,  ..., -1.5357, -1.4672, -1.3473],\n",
      "          [-0.9877, -1.0048, -1.0219,  ..., -1.5357, -1.5014, -1.4843],\n",
      "          [-0.9877, -1.0048, -1.0219,  ..., -1.4672, -1.5699, -1.6213],\n",
      "          ...,\n",
      "          [-1.0904, -1.0904, -1.1075,  ..., -0.7993, -1.1075, -1.2445],\n",
      "          [-1.0733, -1.0562, -1.0904,  ..., -0.9705, -1.1075, -1.2274],\n",
      "          [-1.1075, -1.0219, -1.0219,  ..., -1.1589, -1.1760, -1.2274]],\n",
      "\n",
      "         [[-0.9503, -0.9678, -0.9853,  ..., -1.4930, -1.4055, -1.3004],\n",
      "          [-0.9328, -0.9503, -0.9678,  ..., -1.5105, -1.4580, -1.4230],\n",
      "          [-0.9503, -0.9503, -0.9853,  ..., -1.4755, -1.5280, -1.5630],\n",
      "          ...,\n",
      "          [-1.0728, -1.0378, -1.0728,  ..., -0.8102, -1.0728, -1.2129],\n",
      "          [-1.0728, -1.0203, -1.0378,  ..., -0.9853, -1.0903, -1.1954],\n",
      "          [-1.0728, -0.9853, -0.9853,  ..., -1.1604, -1.1779, -1.1779]],\n",
      "\n",
      "         [[-0.8807, -0.8807, -0.8981,  ..., -1.3339, -1.2467, -1.1596],\n",
      "          [-0.8458, -0.8807, -0.8981,  ..., -1.3339, -1.3164, -1.2641],\n",
      "          [-0.8633, -0.8807, -0.8981,  ..., -1.2990, -1.3687, -1.3687],\n",
      "          ...,\n",
      "          [-0.9853, -0.9678, -0.9853,  ..., -0.7238, -0.9330, -1.0550],\n",
      "          [-0.9853, -0.9330, -0.9504,  ..., -0.8633, -0.9330, -1.0724],\n",
      "          [-0.9678, -0.9156, -0.8981,  ..., -1.0376, -1.0376, -1.0550]]],\n",
      "\n",
      "\n",
      "        [[[-1.1932, -1.1247, -1.0904,  ..., -1.1932, -1.2788, -1.2617],\n",
      "          [-1.2103, -1.1075, -1.0390,  ..., -1.2274, -1.3302, -1.3130],\n",
      "          [-1.2103, -1.1075, -1.0219,  ..., -1.1932, -1.2959, -1.2445],\n",
      "          ...,\n",
      "          [-1.1075, -1.0904, -1.0904,  ..., -1.2445, -1.2788, -1.1932],\n",
      "          [-1.0733, -1.0733, -1.0733,  ..., -1.3130, -1.4158, -1.5014],\n",
      "          [-1.0904, -1.0733, -1.0562,  ..., -1.3815, -1.4329, -1.5185]],\n",
      "\n",
      "         [[-1.1604, -1.0728, -1.0378,  ..., -1.1604, -1.2479, -1.2129],\n",
      "          [-1.1779, -1.0728, -0.9853,  ..., -1.1779, -1.2479, -1.2654],\n",
      "          [-1.1604, -1.0728, -0.9853,  ..., -1.1604, -1.2479, -1.1954],\n",
      "          ...,\n",
      "          [-1.0553, -1.0378, -1.0378,  ..., -1.2479, -1.2654, -1.2304],\n",
      "          [-1.0028, -1.0378, -1.0028,  ..., -1.2654, -1.3704, -1.4580],\n",
      "          [-1.0203, -1.0378, -1.0028,  ..., -1.3529, -1.3880, -1.4580]],\n",
      "\n",
      "         [[-1.0724, -0.9853, -0.9504,  ..., -1.0550, -1.1247, -1.0898],\n",
      "          [-1.0724, -0.9678, -0.9156,  ..., -1.0724, -1.1247, -1.1421],\n",
      "          [-1.0376, -0.9504, -0.8807,  ..., -1.0376, -1.1247, -1.0898],\n",
      "          ...,\n",
      "          [-0.9504, -0.9330, -0.9330,  ..., -1.0898, -1.1073, -1.1073],\n",
      "          [-0.9156, -0.9330, -0.9156,  ..., -1.1247, -1.2119, -1.3164],\n",
      "          [-0.9330, -0.9330, -0.9156,  ..., -1.1770, -1.2293, -1.2990]]]]), 'labels': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"broad_bean\")\n",
    "# trainer = train_model_of_type_for_crop(\"binary\", \"broad_bean\")\n",
    "\n",
    "# 137 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"common_buckwheat\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"common_buckwheat\")\n",
    "\n",
    "# 207 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"pea\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"pea\")\n",
    "\n",
    "# 403 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"corn\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"corn\")\n",
    "\n",
    "# 303 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"soybean\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"soybean\")\n",
    "\n",
    "# 135 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"sunflower\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"sunflower\")\n",
    "\n",
    "# 410 images\n",
    "# train_model_of_type_for_crop(\"multiclass\", \"sugar_beet\")\n",
    "# train_model_of_type_for_crop(\"binary\", \"sugar_beet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from typing import NoReturn\n",
    "\n",
    "# def shutdown_windows() -> NoReturn:\n",
    "#     subprocess.run([\"shutdown\", \"/s\", \"/t\", \"0\"])\n",
    "\n",
    "# shutdown_windows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
